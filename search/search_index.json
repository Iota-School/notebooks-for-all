{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Astronomy Notebooks for All","text":"<p>Jupyter Notebooks play a central role in modern data science workflows. Despite their importance, these notebooks are inaccessible to people with disabilities, especially those who rely on assistive technology. Impacted users must find extreme workarounds or give up using them entirely. Students with disabilities have reported leaving their field once they learn their chosen career\u2019s foundational tools are inaccessible to them. </p> <p>This is a challenging problem to solve. The Notebooks for All project is taking the first steps, initially focusing on static notebooks:  - Running usability feedback sessions with impacted users who rely on a variety of assistive technology - Capturing what makes notebooks inaccessible with assistive technology, and compiling documents that describe the issues and feedback - Editing notebooks based on the feedback - Organizing events to spread awareness in the scientific community about this issue</p>"},{"location":"#collaborators","title":"Collaborators","text":"<p>Space Telescope Science Institute produces extensive community resources and infrastructure in Jupyter. The Institute has committed to fostering an inclusive environment and has funded this project in 2022-2023 as part of the Director\u2019s Discretionary Fund. Other collaborators include community contributions and work from STEM- and accessibility-focused organizations such as Iota School and Quansight Labs.</p>"},{"location":"#resources","title":"Resources","text":"<p>A Curated List of STScI notebooks Accessibility Analysis of Jupyter Notebook HTML Output Astronomy Notebooks For All full proposal</p>"},{"location":"#license","title":"License","text":"<p>This repository hosts mixed content types. Suitable licenses apply to each type. All of the repository except the <code>[user-tests](user-tests)</code> directory are under a 3-Clause BSD license. All content in the <code>[user-tests](user-tests)</code> directory is under a CC-BY license.</p>"},{"location":"media/","title":"Notebooks for all media","text":"<p>A collection of multimedia references to Notebooks for All.</p>"},{"location":"media/#articles","title":"Articles","text":"<ul> <li>Notebooks For All: Making Jupyter Notebooks More Accessible - Astrobites</li> <li>referenced in [Notably Inaccessible -- Data Driven Understanding of Data Science Notebook Inaccessibility][Data Driven Understanding of Data Science Notebook Inaccessibility]</li> <li>SciPy 2023 written in Japanese</li> </ul>"},{"location":"media/#presentations-and-events","title":"Presentations and Events","text":"<ul> <li>Patrick Smyth, Jenn Kotler Notebooks for All: Accessibility &amp; Jupyter Notebooks | JupyterCon 2023</li> <li>Basic Jupyter Notebook Accessibility</li> <li>arXiv Accessibility Forum 2023 \u2014 Dr. Patrick Smyth: Hackccessibility for All</li> <li>STScI Day of Accessibility playlist <ul> <li>Introduction to Day of Accessibility</li> <li>Notebooks for All</li> <li>Illuminating the Universe with Accessible Text Descriptions</li> <li>Joshua Miele - Day of Accessibility Keynote</li> <li>Hearing the Light - Astronomy Data Sonification</li> <li>Learn to Use a Screen Reader</li> </ul> </li> <li>Accessibility best practices for authoring Jupyter notebooks @ SciPy 2023</li> </ul>"},{"location":"media/#steams","title":"Steams","text":"<ul> <li>@smythp &amp; @tonyfast review the <code>nbconvert-a11y</code> template.<sup>1</sup></li> <li>10/4/2023 - https://www.youtube.com/watch?v=sv5l5bVNE20</li> <li>10/18/2023 - https://www.youtube.com/watch?v=GkINFX3aJw0</li> <li>10/25/2023 - https://www.youtube.com/watch?v=Ku-leCdn6hk</li> </ul> <ol> <li> <p>these videos proved to be a more effective way to understand the assistive experience users are having. we learned that videos should be considered when reviewing open source issues with a screen reader. it helps to watch these videos sped up, so expert screen reader users can help developers by slowing down their reader for better cognition.\u00a0\u21a9</p> </li> </ol>"},{"location":"nbconvert_html5/","title":"the <code>html5</code> template","text":"<p>an <code>nbconvert</code> template designed for an accessible experience when rendering notebooks as html webpages. more generally, it could serve as an accessible substrate to build computational literature with like documentation, research papers, or blog posts.</p> <p><code>jupyter nbconvert --to html5</code> features:</p> <ul> <li> semantic html tags, roles, and aria for the notebook and its cells</li> <li> efficient tab navigation including:</li> <li> skips links</li> <li> heading links with large hit areas</li> <li> cell source as <code>readonly</code> forms that take tab focus</li> <li> any other rich interactive content in the <code>output</code></li> <li> uses Atkinson Hyperlegible which is specifically to increase legibility for readers with low vision, and to improve comprehension. </li> <li> uses the <code>github-light-colorblind</code> <code>pygments code theme from the [accessible-pygments](https://github.com/Quansight-Labs/accessible-pygments) project based on [</code>a11y-syntax-highlighting`](https://github.com/ericwbailey/a11y-syntax-highlighting)</li> <li> screen reader landmarks, headings (markdown &amp; outputs), forms (cell inputs), and table navigation</li> <li> operable when zoomed in</li> <li> table of contents for code and narrative navigation</li> <li> configurable accessibility settings</li> <li> persistent settings across sessions</li> <li> best practice auditting during conversion</li> <li> automated remediations</li> <li> fix rendered pandas tables</li> </ul>"},{"location":"nbconvert_html5/#template-scope","title":"template scope","text":"<p>the template defines the majority of the web page from the <code>html</code> tag to the cell outputs. every element is defined using a meaningful tag or aria role. the cell outputs come from user land and our template can't control their content. if author's abide some best practices then they can ensure an accessible experience when their notebook is exported to html.</p>"},{"location":"nbconvert_html5/#pour-caf-principles","title":"POUR-CAF principles","text":"<p>notebooks often harness data visualizations. their mission co-develops with accessible visualizations. this project goals beyond the standard WCAG POUR principles and adds Chartability's CAF principles and heuristics to the design.</p>"},{"location":"nbconvert_html5/#a-table-of-cells","title":"a table of cells","text":"<p>this template represents a notebook as a html table where each notebook cell is a row in the html. the table pattern is a natural html pattern and adds a new dimension to screen readers navigating notebook documents.</p>"},{"location":"nbconvert_html5/#table-of-contents-navigation","title":"table of contents navigation","text":"<p>notebook documents can be long and navigating them need to be easier.</p> <ul> <li>Esc - minimizes the table of contents</li> <li>Ctrl + Esc - toggles the table of contents</li> </ul>"},{"location":"nbconvert_html5/#conclusion","title":"conclusion","text":"<p>the html version of notebooks is not the same interactive state as the editting experience, but it is still a highly interactive experience. overall, focusing on an accessible substrate to build sites from has improved the experience from abled and disabled people.</p>"},{"location":"readme/","title":"Astronomy Notebooks for All","text":"<p>Jupyter Notebooks play a central role in modern data science workflows. Despite their importance, these notebooks are inaccessible to people with disabilities, especially those who rely on assistive technology. Impacted users must find extreme workarounds or give up using them entirely. Students with disabilities have reported leaving their field once they learn their chosen career\u2019s foundational tools are inaccessible to them. </p> <p>This is a challenging problem to solve. The Notebooks for All project is taking the first steps, initially focusing on static notebooks:  - Running usability feedback sessions with impacted users who rely on a variety of assistive technology - Capturing what makes notebooks inaccessible with assistive technology, and compiling documents that describe the issues and feedback - Editing notebooks based on the feedback - Organizing events to spread awareness in the scientific community about this issue</p>"},{"location":"readme/#collaborators","title":"Collaborators","text":"<p>Space Telescope Science Institute produces extensive community resources and infrastructure in Jupyter. The Institute has committed to fostering an inclusive environment and has funded this project in 2022-2023 as part of the Director\u2019s Discretionary Fund. Other collaborators include community contributions and work from STEM- and accessibility-focused organizations such as Iota School and Quansight Labs.</p>"},{"location":"readme/#resources","title":"Resources","text":"<p>A Curated List of STScI notebooks Accessibility Analysis of Jupyter Notebook HTML Output Astronomy Notebooks For All full proposal</p>"},{"location":"readme/#license","title":"License","text":"<p>This repository hosts mixed content types. Suitable licenses apply to each type. All of the repository except the <code>[user-tests](user-tests)</code> directory are under a 3-Clause BSD license. All content in the <code>[user-tests](user-tests)</code> directory is under a CC-BY license.</p>"},{"location":"readme/#events","title":"events","text":"<ul> <li>2023.03.10 Accessible Notebooks Hackathon</li> <li>2023.04.13 Day of Accessibility host by the Space Telescope Institute</li> </ul>"},{"location":"exports/configs/","title":"configuration files","text":"<ul> <li>default.py</li> <li>a11y.py</li> </ul>"},{"location":"exports/notebooks/","title":"reference notebooks","text":"<ul> <li>lorenz-executed.ipynb</li> <li>stsci_example_notebook.ipynb</li> <li>Imaging_Sky_Background_Estimation.ipynb</li> <li>lorenz.ipynb</li> </ul>"},{"location":"exports/resources/proposal-astronomy-notebooks-for-all/","title":"Astronomy Notebooks for All","text":"<p>PI: Jennifer Kotler </p> <p>Co-I\u2019s: Erik Tollerud, Isabela Presedo-Floyd (Quansight Labs),  Tony Fast (Quansight Labs), Patrick Smyth (Iota School) </p> <p>Budget: $86,115 | With internal costs: $102,531</p>"},{"location":"exports/resources/proposal-astronomy-notebooks-for-all/#abstract","title":"Abstract","text":"<p>This proposal will enable scientists who rely on assistive technology (ie: screen readers) to use Jupyter Notebooks, and for STScI to engage this community through educational events. We will accomplish these objectives through four activities: usability testing, implementing accessibility enhancements, contributing to the open software ecosystem, and education.</p>"},{"location":"exports/resources/proposal-astronomy-notebooks-for-all/#justification","title":"Justification","text":"<p>STScI builds many community resources using Jupyter notebooks: some pages are \u201cnotebook first\u201d like the JWST Data Analysis Notebooks, MAST\u2019s notebooks, or the COS tutorials, others are attached to other efforts like the POPPY demo notebooks, the JWebbinar notebooks, or the WFC3 tools. Additionally, STScI science staff use notebooks for critical aspects of their research. Thus, Project Jupyter plays a central role in modern data science workflows. Despite this importance, these notebooks are inaccessible to people with disabilities, especially those who rely on assistive technology. Impacted users must find extreme workarounds or give up using them entirely. Students with disabilities have reported leaving their field once they learn their chosen career\u2019s foundational tools are inaccessible to them. STScI contributes to this problem by producing extensive inaccessible resources and infrastructure in Jupyter. The Institute has committed to fostering an inclusive environment; as a government contractor our work must comply with standards of accessibility laid out in legislation such as the Americans with Disabilities Act (ADA). This problem must be solved!</p> <p>Notebooks have had this issue for many years, but as an open-source community driven project, Project Jupyter has not made meaningful progress towards solving it. Resolving accessibility barriers is not a simple task that can be ticketed and fixed by the community alone. A user-centered approach with iterative usability testing and development is needed to make notebooks accessible. This is where STScI can play a vital role, leveraging our expertise gained from previous work on sonification and accessibility. </p> <p></p> <p>Please watch this video recorded by Smyth for a more detailed explanation of notebook accessibility when rendered in the browser: Accessibility Analysis of Jupyter Notebook HTML Output</p> <p>By following this user-centered approach and by liaising with Project Jupyter to contribute improvements, STScI will demonstrate leadership through a timely, high-impact contribution to open source software in line with our core values of inclusion, open research, and legacy: making tools accessible for the next generation of astronomers. Contributing this work to Project Jupyter will reduce any need for the Institute to continue maintaining the notebook rendering mechanism. Upstreamed improvements will benefit both STScI users and the community of almost one million data scientists and researchers who render notebooks to share their research. Running educational events that teach accessibility concepts to technical, research, and administrative stakeholders at STScI will help to crystallize and share insights developed during the project and pave the way for future initiatives related to inclusion at the Institute. </p>"},{"location":"exports/resources/proposal-astronomy-notebooks-for-all/#project-scope","title":"Project Scope","text":"<p>To improve accessibility of notebooks rendered in STScI notebook repositories, we will focus on four activities: usability testing, implementation of accessibility enhancements, education, and community contribution. These stages may not happen in order and will inform each other.</p>"},{"location":"exports/resources/proposal-astronomy-notebooks-for-all/#usability-testing","title":"Usability Testing","text":"<p>Many accessibility initiatives put standards before the feedback of people with disabilities, resulting in surface-level improvements that do not create more inclusive communities or accessible tools. We will follow an inclusive and participatory approach focusing on learning impacted users\u2019 preferred experience in resolving accessibility issues. The usability tests will broadly focus on common tasks for working with rendered notebooks, such as navigating through many layers of content and ensuring appropriate assistive settings are available. </p> <p>We will recruit impacted users to participate in the usability tests. Testers will be paid for their time at an industry-standard rate. Further input may be provided asynchronously by community members writing to us about their experience. While not our primary source of feedback, we aim to share developments on a public GitHub repository, opening the project to a broader pool of contributors from open-source, accessibility, and STEM communities. We intend to extend this spirit of inclusion to our workshops, reserving space for members of affected communities.</p> <p>Usability experts Kotler and Presedo-Floyd will facilitate the sessions remotely through Zoom (used for accessible testing in the Astronify  DDRF). If accessibility testers live within easy travel of a team member and current public health guidelines allow, some sessions may be scheduled in-person. Tests will be recorded so they can be reviewed later internally. A technical team member will attend each session to troubleshoot problems that interfere with testing. If an issue is straightforward to address, they may make small fixes in real time to iterate on the solution with the tester. Larger issues will be recorded as a ticket and addressed later. We will meet with testers again between development iterations to incorporate further feedback.</p>"},{"location":"exports/resources/proposal-astronomy-notebooks-for-all/#implementing-accessibility-enhancements","title":"Implementing accessibility enhancements","text":"<p>We will focus on implementing an accessible notebook template, the precursor to how notebooks are rendered in browsers. This provides a high value-to-effort ratio because very few templates are automatically applied to large numbers of notebooks at both STScI and in the wider scientific community. We will test our templates with a notebook that combines the Jupyter team benchmark notebooks, and a straightforward science case from STScI\u2019s public notebook collection (either JDAT notebooks or the general notebooks. This testing notebook will be run through the Jupyter\u2019s nbconvert machinery (the backend used for STScI\u2019s notebook repositories and the public nbviewer website) to produce static web pages of these notebooks. To ensure testing covers the needs of our scientists, the test notebook will have the variety of cell types (image, text, code, etc.), outputs, and interactive widgets found in ST notebooks. We will conduct development and usability testing as follows:</p> <ol> <li>We will conduct an initial usability test to identify the most severe accessibility issues, informing initial development work.</li> <li>We will follow with two more cycles of iterative testing of updated notebooks to better identify user-desired behavior. These cycles of feedback and development will result in an accessible notebook template that emphasizes an enjoyable accessibility experience.</li> </ol>"},{"location":"exports/resources/proposal-astronomy-notebooks-for-all/#contributing-to-the-open-software-ecosystem","title":"Contributing to the Open Software Ecosystem","text":"<p>Once we create an accessible rendered notebook template for STScI projects, we will share it with the wider Jupyter project targeting nbviewer and nbconvert ensuring that all future notebooks are rendered accessible by default. By merging this template in nbconvert this work will be maintained by the Jupyter community\u2014no further STScI maintenance required. We will also produce a report summarizing the findings from the usability research describing the accessibility enhancements for rendered notebooks. This document will provide a valuable foundation for future revisions to the schema so that all notebooks, not only those rendered in HTML, will be more accessible in the future.</p>"},{"location":"exports/resources/proposal-astronomy-notebooks-for-all/#education-accessibility-events","title":"Education - Accessibility Events","text":"<p>By developing an awareness of accessibility issues, STScI researchers, administrators, and technologists can become leaders in the creation of inclusive technical infrastructure and resources. We propose having Smyth of Iota School host two hybrid, educational training days to develop an understanding of inclusive practices related to accessibility at STScI:</p> <ol> <li>STScI Day of Accessibility: This full-day educational event will combine engaging activities teaching accessibility skills and talks by STEM researchers with disabilities.</li> <li>A11Y Hackathon: In this full-day accessibility hackathon, participants will contribute to public resources and documentation created by STScI by adding alt text to images, transcribing recordings, and checking documents for common accessibility issues.</li> </ol>"},{"location":"exports/resources/proposal-astronomy-notebooks-for-all/#work-plan","title":"Work Plan","text":"<p>Our team is well suited to achieve the goals of our proposal. We are subject matter experts in usability testing, development, Jupyter, screen readers, and accessibility. A vital part of this proposal is the inclusion of Patrick Smyth, Chief Learner at Iota School. Patrick is a blind developer and research computing specialist with ties to blind and visually impaired coding communities. Iota School offers inclusive technical and accessibility training to research organizations. Patrick will provide expertise in development, user testing, and educational events. Kotler has run usability tests with blind programmers and designed accessible software through her work on Astronify and MAST. Fast and Presedo-Floyd of Quansight are entrenched Jupyter community members with cross-project knowledge and connections to other major contributors. They have spent over a year focused on accessibility within Jupyter, from community outreach work to direct contributions. Their community knowledge will enable us to use momentum from existing Jupyter accessibility efforts towards the success of this proposal.</p> Time Period Activity May-June 2022 Write usability testing script, Recruit testers July-August 2022 Run first round of tests, Host STScI Day of Accessibility in person August &amp; September \u201822 Design and implement notebook adjustments October-December \u201822 Run more tests as needed, Host A11Y Hackathon December \u201822 &amp; January \u201823 Design and implement more notebook adjustments February-March \u201823 Finalize a template with our accessibility updates April \u201823 Update the ST notebooks &amp; internal machinery, Propose adding the accessible templates upstream to nbconvert to improve research accessibility in the broader scientific community"},{"location":"exports/resources/proposal-astronomy-notebooks-for-all/#final-outcomes-deliverables-program-impact","title":"Final Outcomes - Deliverables &amp; Program Impact","text":"<p>This grant would culminate in four major deliverables: </p> <ol> <li>An accessible notebook template that improves the experience of notebooks rendered in the browser, especially for people who use assistive tech. The STScI community and far beyond will be impacted\u2014millions of notebooks are accessed through browsers monthly.</li> <li>An accessible benchmark notebook that can be used to test future ST software utilizing notebooks for accessibility</li> <li>A report of our usability testing findings. These lessons learned will be a helpful reference for other Jupyter projects on how to make the notebook file, interfaces, or other projects accessible. In particular, this document will assess the feasibility of accessibility work on the Jupyter notebook editor, which may serve as a basis for future high impact accessibility work funded through inter-organizational grant initiatives.</li> <li>Two accessibility education events to raise awareness of accessibility in STEM</li> </ol>"},{"location":"exports/resources/proposal-astronomy-notebooks-for-all/#previous-ddrf-awards","title":"Previous DDRF Awards","text":"<p>Kotler - Create with Light - High School Artist Grant, PI (completed 2022)</p> <p>Tollerud - Community Software Initiative, science PI (completed 2020)</p>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/","title":"Accessibility Tips for Creating &amp; Publishing Jupyter Notebooks","text":"<p>By the Notebooks for all team</p> <p>Draft 2: February 21, 2023</p> <p>The accessibility of Jupyter Notebooks is determined by many factors, many not in the direct control of notebook authors. For example, specific libraries may create outputs that are not accessible to screen readers, and default export options may create outputs that have issues related to contrast or keyboard navigation. Further, it is important to note that, currently, editing Jupyter Notebooks is largely not accessible for screen reader users.</p> <p>Despite these serious challenges, there are ways that notebook authors can create notebooks that are more accessible for users with disabilities. This document will give context on accessibility in Jupyter Notebooks and provide some tips and best practices for authoring and publishing accessible notebooks.</p>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#notebook-formats","title":"Notebook Formats","text":"<p>Depending on the export method and content, notebooks can allow for different levels of accessibility for people using assistive technology. Users encounter notebooks in a variety of formats:</p> <ul> <li>The editable notebook format \u2014 This format is designed to be opened directly in the Jupyter environment for editing and take the form of <code>.ipynb</code> files. This format is currently inaccessible to screen reader users and has many obstacles to navigating the UI and editing cells</li> <li>Uneditable notebooks exported to HTML \u2014 These <code>.html</code> files created through the <code>nb-convert</code> exporter are designed to be opened in a browser and are often shared on the web. This format is somewhat accessible because HTML is built for web accessibility. While there are issues, people tend to succeed in accessing a majority of notebooks exported in this format.</li> </ul> <p>The interactive notebook format still does not play well with assistive technology, particularly in navigating the UI and editing cells. The exported \u201cread only\u201d notebooks can be read fairly well, but still have some issues. If you want your content to be 100% accessible, Jupyter notebooks are currently not the best way to publish your content (though they can be converted to HTML for a better read-only accessibility experience). Consider if there is an alternate format that could work just as well, or publish a \u201cread only\u201d export alongside the editable one so at least the content is readable by everyone. That said, things will not be truly accessible until we provide access to content creation, not just knowledge consumption.</p> <p>There is work to be done to improve the way Jupyter Notebooks are exported to HTML, and we are running tests with affected users to figure out what changes would best improve accessibility in this area. Eventually, we hope to push changes and improvements upstream. This is a slow process with not enough people working on it. We have several open issues and hopefully our team and others will make improvements in the next few years. But that does not solve the problem right now.</p> <p>Since editable notebooks published as <code>.ipynb</code> files currently have serious issues related to accessibility, there is little that notebook authors can do to help people with disabilities access this format. However, the accessibility of HTML notebooks can be significantly improved by using specific best practices. By following the below tips, you can act to make a difference in the accessibility of your notebooks.</p>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#future-plans-and-next-steps","title":"Future Plans and Next Steps","text":"<ul> <li>We will be publishing a best practice document for authoring Jupyter Notebooks</li> <li>Attend the accessible notebook hackathon we are running to practice the tips included below on a notebook you intend to publish (March 10th 10-12:30 EST)</li> <li>We will publish a read only notebook format (correct word?)  available on github that has improved navigation, color contrast, etc which you can apply to your static NB viewer notebooks before publishing</li> </ul>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#authoring-tips-draft","title":"Authoring Tips (Draft)","text":""},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#use-well-formatted-markdown","title":"Use well-formatted Markdown","text":"<p>Use content headings. There is only one H1. Do not skip heading levels.</p>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#all-text-in-the-document-needs-to-appear-as-plain-text","title":"All text in the document needs to appear as plain text","text":"<ul> <li>If there\u2019s text in an image, in a chart, in a video, in an audio recording, or other relatively inaccessible formats, it should also be in plain text somewhere. There are multiple ways you can handle this depending on content and context.</li> <li>Options for providing information in plain text alongside other formats include: adding a description, including a caption, or describing everything fully in surrounding paragraphs.</li> </ul>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#visualization-accessibility","title":"Visualization Accessibility","text":"<ul> <li>Include titles in visualizations, such as the outputs from libraries such as Matplotlib. Both in plot and as a property.</li> <li>Label visualization axes, include keys/legends </li> <li>They should have good contrast (the relative difference in tonal hues). Be cautious of low opacity and thin lines, or color choices that are too similar, such as light gray on white. Try https://github.com/Quansight-Labs/accessible-pygments </li> <li>Consider plotting in only black and white. You can always add color later.</li> <li>Don\u2019t rely only on color to convey information. Include labels. Consider using a mix of color and patterns to differentiate values.</li> </ul>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#descriptive-text-for-visual-areas","title":"Descriptive text for visual areas","text":"<ul> <li>When using an image with an .img tag in the HTML, alt text may be used normally</li> <li>When creating a plot or graph, some libraries allow alt text and others don\u2019t. Captions and titles should be used to fill in information that alt text would normally contain when there is no option for it. </li> <li>Legends, Axis labels, numbered tic-marks, and other text in generated graphs cannot currently be read by a screen reader and may be too small for low vision people to find with magnification. </li> <li>When writing alt text for a plot, Include: <ul> <li>Type of Plot (bar chart, image, scatter-plot,etc)</li> <li>Title of graph</li> <li>Axis labels and range</li> <li>Key / legend</li> <li>General explanation of graph and what it is communicating</li> </ul> </li> <li>It can be very helpful to link to a file containing the original data, or if possible include a data table near the plot so it can be accessed in a non visual way</li> <li>Include a sonification for a plot. You can use Astronify to do this for time series and spectral astronomy data.</li> </ul>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#organization","title":"Organization","text":"<p>To help people orient themselves in the notebook and understand what to expect, give some context at the beginning. We recommend: * Give the document a title. This should be the one H1 you use, and it should be at the top. * Include a summary of the document under the title. * Add a table of contents as an ordered list (even if it cannot contain links) * Add the author and affiliation, where relevant * Include information such as the date and time first published and the date and time last edited. * Link to the notebook source, where it can be used in editable form</p>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#color-contrast","title":"Color contrast","text":"<ul> <li>Color Contrast for in[] out[] can be templated to be higher contrast, standard does not read well for low vision</li> <li>Use a syntax highlighting theme that considers accessible contrast (examples at ericwbailey/a11y-syntax-highlighting)</li> </ul>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#use-plain-language","title":"Use \u201cplain language\u201d","text":"<ul> <li>Define acronyms the first time you use them and use them sparingly</li> <li>Only use field-specific terms when needed. Use approachable language when the terms aren\u2019t critical to understanding the rest of the document or related literature.</li> <li>More general tips on stylistic choices with accessibility considerations can be found on Google\u2019s developer documentation style guide.</li> </ul>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#use-descriptive-link-names","title":"Use descriptive link names!","text":"<ul> <li>Do not: Click here!</li> <li>Do: Learn more at Space Telescope</li> </ul>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#lead-into-your-code-cells-where-relevant","title":"Lead into your code cells (where relevant)","text":"<ul> <li>Make sure they are under other content headings (ie. an imports cell can be preceded by a markdown cell with a header \u201cImports\u201d)</li> <li>Tell what the cell should do before it is done. Usually this is in a markdown cell before, but it also could be a comment in the code cell.</li> <li>Do not list several different ways someone could complete the task unless that is the point of the notebook. Focus on what you are doing first, and mention alternates later if needed.</li> </ul>"},{"location":"exports/resources/event-hackathon/accessibility-tips-for-jupyter-notebooks/#comment-on-your-code-where-relevant","title":"Comment on your code (where relevant)","text":"<ul> <li>This is especially important for long stretches of code or where more specificity is needed. </li> </ul>"},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/","title":"Notebook authoring accessibility checklist","text":"<p>How usable a Jupyter notebook is for disabled people can vary widely\u2014-it depends on how the document is set up and presented. Notebook authors can use writing and publishing techniques to ensure their notebooks' content is more accessible.</p> <p>Use this checklist to start reviewing notebooks. Editing your notebook for accessibility is an excellent step you can take to make our communities more inclusive!</p> <p>This checklist is based on the draft authoring tips from the Notebooks For All team.</p> Table of contents   - [Notebook authoring accessibility checklist](#notebook-authoring-accessibility-checklist)   - [Structure](#structure)     - [The First Cell](#the-first-cell)     - [The Rest of the Cells](#the-rest-of-the-cells)   - [Text](#text)   - [Code](#code)   - [Media](#media)     - [Images](#images)     - [Visualizations](#visualizations)     - [Videos](#videos)     - [Audio and Sonifications](#audio-and-sonifications)     - [Interactive Widgets](#interactive-widgets)   - [Related resources](#related-resources)   About this document last updated on March 15, 2023 <p>Authors: * Isabela Presedo-Floyd @isabela-pf * Jenn Kotler @jenneh * Patrick Smyth @smythp * Tony Fast @tonyfast * Erik Tollerud @eteq</p> <p>Originally published on February 28, 2023</p>"},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/#structure","title":"Structure","text":"<p>Author should be aware of information, structure, and relationships in their authored works. </p>"},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/#the-first-cell","title":"The First Cell","text":"<ul> <li> The title of the notebook in a first-level heading (eg. <code>&lt;h1&gt;</code> or <code># in markdown</code>).</li> <li> A brief description of the notebook.</li> <li> A title in the form of a H1 (<code>#</code> in Markdown).</li> <li> A brief summary of the notebook.</li> <li> A table of contents in an ordered list (<code>1., 2.,</code> etc. in Markdown).</li> <li> The author(s) and affiliation(s) (if relevant).</li> <li> The date first published.</li> <li> The date last edited (if relevant).</li> <li> A link to the notebook's source(s) (if relevant).</li> <li> The author(s) and affiliation(s) (if relevant).</li> <li> A table of contents in an ordered list (<code>1., 2.,</code> etc. in Markdown).</li> </ul>"},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/#the-rest-of-the-cells","title":"The Rest of the Cells","text":"<ul> <li> There is only one H1 (<code>#</code> in Markdown) used in the notebook.</li> <li> The notebook uses other heading tags in order (meaning it does not skip numbers).</li> </ul>"},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/#text","title":"Text","text":"<ul> <li> All link text is descriptive. It tells users where they will be taken if they open the link.</li> <li> Use plain language wherever possible.</li> <li> All acronyms are defined at least the first time they are used. </li> <li> Field-specific/specialized terms are used when needed, but not excessively.</li> <li> Text is broken into paragraphs and/or cells where relevant.</li> <li> Text is in complete sentences where relevant.</li> </ul>"},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/#code","title":"Code","text":"<ul> <li> Code sections are introduced and explained before they appear in the notebook. This can be fulfilled with a heading in a prior Markdown cell, a sentence preceding it, or a code comment in the code section.</li> <li> Code has explanatory comments (if relevant). This is most important for long sections of code.</li> <li> If the author has control over the syntax highlighting theme in the notebook, that theme has enough color contrast to be legible.</li> <li> Code and code explanations focus on one task at a time. Unless comparison is the point of the notebook, only one method for completing the task is described at a time.</li> </ul>"},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/#media","title":"Media","text":"<p>This list is not exhaustive. If you are reviewing a notebook with content that you do not think fits any of these categories, keep in mind</p> <ul> <li>Text is flexible. Whether it is in the document or linked out, text can be read visually, be read audibly, be magnified, or be translated to another language. Having a text alternative is a good back up plan.</li> <li>Having enough color contrast is required on almost all visual content.</li> </ul>"},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/#images","title":"Images","text":"<ul> <li> <p> All images (jpg, png, svgs) have an image description. This could be</p> <ul> <li> Alt text (an <code>alt</code> property) </li> <li> Empty alt text for decorative images/images meant to be skipped (an <code>alt</code> attribute with no value)</li> <li> Captions</li> <li> If no other options will work, the image is decribed in surrounding paragraphs.</li> </ul> </li> <li> <p> Any text present in images exists in a text form outside of the image (this can be alt text, captions, or surrounding text.)</p> </li> </ul>"},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/#visualizations","title":"Visualizations","text":"Expand the visualizations checklist  - [ ] All visualizations have an image description. Review the previous section, Images, for more information on how to add it. - [ ] [Visualization descriptions](http://diagramcenter.org/specific-guidelines-e.html) include     - [ ] The type of visualization (like bar chart, scatter plot, etc.)     - [ ] Title     - [ ] Axis labels and range     - [ ] Key or legend     - [ ] An explanation of the visualization's significance to the notebook (like the trend, an outlier in the data, what the author learned from it, etc.)  - [ ] All visualizations have the following labels     - [ ] Title     - [ ] Labels on all axes     - [ ] Key or legend (if relevant)  - [ ] All visualizations and their parts have [enough color contrast](https://www.w3.org/WAI/WCAG21/Understanding/contrast-minimum.html) ([color contrast checker](https://webaim.org/resources/contrastchecker/)) to be legible. Remember that transparent colors have lower contrast than their opaque versions. - [ ] All visualizations [convey information with more visual cues than color coding](https://www.w3.org/WAI/WCAG21/Understanding/use-of-color.html). Use text labels, patterns, or icons alongside color to achieve this. - [ ] All visualizations have an additional way for notebook readers to access the information. Linking to the original data, including a table of the data in the same notebook, or sonifying the plot are all options."},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/#videos","title":"Videos","text":"Expand the videos checklist   - [ ] All videos have titles in the player or in the text before them. - [ ] All videos have [captions/subtitles](https://www.w3.org/WAI/media/av/captions/). This can include visual information descriptions if relevant. - [ ] All videos have [transcripts](https://www.w3.org/WAI/media/av/transcripts/). This can include visual information descriptions if relevant. - [ ] All [video players](https://www.w3.org/WAI/media/av/player/) have buttons with labels. This can be a persistent label or appear when hovered. - [ ] All video players have buttons with [enough color contrast](https://www.w3.org/WAI/WCAG21/Understanding/non-text-contrast.html). - [ ] No videos have [flashing images at more than three frames per second](https://www.w3.org/WAI/WCAG21/Understanding/three-flashes-or-below-threshold.html)."},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/#audio-and-sonifications","title":"Audio and Sonifications","text":"Expand the audio and sonifications checklist  - [ ] Sonifications include a key explaining the mapping of data to sound. A written description can be used to convey this information. - [ ] Sonification outputs reference the method that generated the sonification. This can be done in a code cell or with a link to the file used to generate the sonification. - [ ] Audio players include basic listening controls for starting, pausing, volume, and speed. - [ ] All audio players have buttons with labels. This can be a persistent label or appear when hovered. - [ ] All audio players have buttons with [enough color contrast](https://www.w3.org/WAI/WCAG21/Understanding/non-text-contrast.html)."},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/#interactive-widgets","title":"Interactive Widgets","text":"Expand the interactive widgets checklist   The accessibility of interactive widgets varies greatly depending how they are included in the notebook. Review beyond this checklist may be needed.  - [ ] All interactive widgets with visual controls have labels. This can be a persistent label or appear when hovered. - [ ]  All interactive widgets with visual controls have [enough color contrast](https://www.w3.org/WAI/WCAG21/Understanding/non-text-contrast.html). - [ ] All interactive widgets have a summary of what they are and what they do in the surrounding text. - [ ] If an interactive widget's contents are needed to understand the rest of the notebook, the widget either needs to be tested further or have that content fully represented not as a widget elsewhere in the notebook."},{"location":"exports/resources/event-hackathon/notebook-authoring-checklist/#related-resources","title":"Related resources","text":"<p>Resources are for information purposes only, no endorsement implied.</p> <ul> <li>Markdown Cheat Sheet</li> <li>An <code>alt</code> Decision Tree - World Wide Web Consortium Web Accessibility Initiative</li> <li>Image Description Guidelines - DIAGRAM Center</li> <li>Alt text style guide for Jupyter accessibility workshops</li> <li>Contrast Checker - Web Accessibility In Mind</li> <li>Accessible syntax highlighting themes. a11y-syntax-highlighting by Eric Bailey on GitHub</li> <li>Accessible visualization guidelines. Chartability</li> <li>A package for sonifying astronomical data. Astronify</li> <li>Quick accessibility tests anyone can do - Tetralogical blog</li> <li>Other Notebooks For All resources</li> <li>Web Content Accessibility Guidelines (WCAG) 2.1</li> <li>Guidance for writing, designing, and developing for accessibility.Design and Develop Overview - World Wide Web Consortium Web Accessibility Initiative</li> <li>Guidance and tools for digital accessibility - GOV.UK</li> <li>Accessibility for Teams - United States General Services Administration</li> </ul>"},{"location":"exports/user-tests/","title":"User testing information","text":"<p>User testing on this project was led by Jenn Kotler and Isabela Presedo-Floyd. Resources in this directory were created collaboratively.</p> <p>Because this directory holds mostly non-code content, it is licensed under a CC-BY 4.0 license.</p>"},{"location":"exports/user-tests/#test-support","title":"Test support","text":"<p>Resources not tied to any single test round.</p> <ul> <li>Generic recording consent example</li> <li>Outreach and recruitment email template</li> <li>Possible test notebooks</li> <li>User testing resources</li> </ul>"},{"location":"exports/user-tests/#test-1-navigation","title":"Test 1: Navigation","text":"<p>August 2022. How do users navigate and access different levels of structure within HTML outputs of Jupyter notebooks?</p> <ul> <li>Test 1 script</li> <li>Test 1 notebook. Originally sourced from the STScI notebook style guide template. This is a template notebook for creating STScI tutorials with a mix of instruction and example.</li> <li>Test 1 results</li> </ul>"},{"location":"exports/user-tests/#test-2-content-types","title":"Test 2: Content types","text":"<p>November 2022\u2013January 2023. How are users able to access a range of commonly used content types within HTML outputs of Jupyter notebooks?</p> <ul> <li>Test 2 script</li> <li>Test 2 notebook. Originally sourced from the STScI JDAT notebooks' Imaging Sky Background Estimation notebook. This notebook is part of a STScI's post-pipeline Data Analysis Tools Ecosystem and demonstrates how to estimate the sky background in complex scenes and evaluate the quality of the sky estimation. It was modified for our tests.</li> <li>Test 2 results</li> </ul>"},{"location":"exports/user-tests/0-support/generic-recording-consent/","title":"Recording consent form","text":"<p>Thank you for participating in our usability research on Space Telescope Science Institute public notebooks.</p> <p>We will be recording your session to allow Space Telescope Science Institute and external grant team members who are unable to be here today to observe your session and benefit from your comments. Please read the statement below and sign where indicated.</p> <ul> <li>I understand that my usability test session will be recorded. </li> <li>I understand that my usability test session recording will be stored on private Space Telescope Science Institute drives.</li> <li>I understand that anonymized summaries of information learned from my session may be publicly shared on the grant repository.</li> <li>I grant Space Telescope Science Institute permission to use this recording for internal use only, for the purpose of improving the designs being tested.</li> </ul> <p>Signature:</p> <p>Print name:</p> <p>Date:</p>"},{"location":"exports/user-tests/0-support/generic-recording-consent/#notes","title":"Notes","text":"<p>Adapted from Steve Krug's Recording consent form.</p> <p>This document was further adapted by the institutions handling all paperwork for participants to meet their own needs. This example is here as a reference for how we collaborated with other teams. </p>"},{"location":"exports/user-tests/0-support/outreach-email-template/","title":"User testing outreach email templates","text":"<p>Subject line: Paid Notebook Accessibility Feedback Session</p>"},{"location":"exports/user-tests/0-support/outreach-email-template/#version-2","title":"Version 2","text":"<p>Hi {{name}},</p> <p>I\u2019m {{sender}}, and I\u2019m reaching out on behalf of the Space Telescope Science Institute and the Project Jupyter community. We\u2019d like to invite you to join us as a paid participant in our upcoming user tests to learn more about Jupyter notebooks and their accessibility needs in {{month}}. We got your contact information from {{referral}}.</p> <p>Our team has been working on improving the inclusivity of our large collection of notebooks and the notebooks Space Telescope scientists will be making in the future. One of our main goals is to evaluate how easy it is to navigate and understand our public non-editable notebooks, and we want your feedback. During the one-hour feedback sessions we\u2019d be asking you to complete some tasks while navigating a preset notebook. We can offer $100 per hour-long session. If you are interested, let me know and I will follow up with scheduling information.</p> <p>Feel free to reach out if you have any other questions. If you are interested in joining us and can\u2019t make that timeline, please let me know. We are planning multiple rounds of user testing and may be able to schedule one that works for you.</p> <p>Best,  {{sender}}</p>"},{"location":"exports/user-tests/0-support/outreach-email-template/#version-2_1","title":"Version 2","text":"<p>Hi {{name}},</p> <p>This is {{sender}} from {{affiliation}}. It\u2019s been awhile since we\u2019ve spoken, I hope you\u2019re doing well!</p> <p>Are you interested in being a paid participant in upcoming user tests about Jupyter notebooks and their accessibility. We can offer $100 per hour-long session. We are aiming to schedule the first round of tests roughly between {{date 1}} and {{date 2}}. </p> <p>If you are interested, please pick a time using this calendy link: </p> <p>If you have trouble with calendy, here\u2019s a link to some accessibility help:  or feel free to respond and tell us what date/times work best for you and we can schedule that way.</p> <p>Please email me with any other questions. If you are interested in joining us and can\u2019t make that timeline, let me know\u2014we are planning multiple rounds of user testing and may be able to schedule one that works for you.</p> <p>Best, {{sender}}</p>"},{"location":"exports/user-tests/0-support/possible-test-notebooks/","title":"Possible test notebooks","text":"<p>A list of existing notebooks that could be used during our rounds of user tests. They may also provide inspiration to custom test notebooks we may make.</p> Notebook or collection name Test candidate? Used in test? (date) JupyterLab examples test.ipynb nbconvert examples Lorenz notebook Yes JupyterLab benchmarks test noteboks Cosmic Origins Spectograph notebooks STScI notebook style guide template Yes (Test 1, August 2022) STScI notebook style guide Yes STScI JDAT notebooks (Rendered here) STScI JWebbinar notebooks Maybe (needs content review) <p>The following are axes we considered choosing notebooks based on. If they end up being a deciding factor we take note of, then they will be added to the above table.</p> <ul> <li>length (in cells)</li> <li>has MD cells</li> <li>has code cells</li> <li>has text-only outputs</li> <li>has non-text outputs</li> <li>notes about outputs</li> <li>uses STScI data</li> <li>follows STScI notebook guidelines</li> <li>renders successfully (in viewer tool)</li> <li>Test hosts can explain it</li> </ul>"},{"location":"exports/user-tests/0-support/user-testing-resources/","title":"User testing resources for notebooks","text":"<p>Astronomy Notebooks for All - STScI</p>"},{"location":"exports/user-tests/0-support/user-testing-resources/#general-user-testing-resources","title":"General user testing resources","text":"<p>Steve Krug usability testing guides - Usability test script - Recording consent form - Usability Testing Checklists - \u201cThings a therapist would say\u201d</p> <p>Remote Usability Testing: Study Guide - Nielsen Norman Group</p> <p>How you can perform cheap unmoderated usability testing using Zoom - UX Collective</p>"},{"location":"exports/user-tests/0-support/user-testing-resources/#test-specific-resources","title":"Test-specific resources","text":"<ul> <li>jupyter/nbformat</li> <li>jupyter/nbconvert</li> <li>jupyter/nbviewer<ul> <li>What is the STScI equivalent for this?</li> </ul> </li> <li>jupyter/nbconvert-examples</li> <li>JupyterLab UI overview - Adobe Experience League</li> <li>Some jupyterlab/jupyterlab user stories</li> <li>The Lorenz Notebook</li> </ul>"},{"location":"exports/user-tests/0-support/user-testing-resources/#accessibility","title":"Accessibility","text":"<ul> <li>Browsing with assistive technology videos</li> <li>Remotely Co-Designing Features for Communication Applications using Automatic Captioning with Deaf and Hearing Pairs</li> <li>CAIR Lab - RIT</li> </ul>"},{"location":"exports/user-tests/1-navigation/results/","title":"Results: Structure &amp; Navigation in Rendered Notebooks","text":"<p>These results are from user interviews conducted in August 2022 with the navigation test script on the STScI tutorial sample notebook.</p>"},{"location":"exports/user-tests/1-navigation/results/#what-we-tested","title":"What we tested","text":"<p>Operating systems: Mac OS Monterey, Windows 10</p> <p>Browsers: Chrome, Firefox, Safari</p> <p>Assistive tech: JAWS (screen reader), NVDA (screen reader), VoiceOver (screen reader), Mantis (braille reader), Mac OS Zoom (built-in screen magnifier), built-in browser zoom controls, built-in large cursor and pointer settings</p> <p>Interface: Browser and notebook in HTML form via nbconvert(hosted via GitHub pages)</p> <p>Sample size: 6 participants</p> <p>Method: Combination of qualitative usability testing and user interviews </p>"},{"location":"exports/user-tests/1-navigation/results/#how-users-navigated","title":"How users navigated","text":"<p>The following sections describe the ways participants chose to navigate through a single notebook (one browser page). These methods are likely not notebook-specific, but they have only been noted in that context. They are not listed in any particular order.</p>"},{"location":"exports/user-tests/1-navigation/results/#by-headings","title":"By headings","text":"<p>This navigation method appeared exclusively with participants using screen readers.</p> <ol> <li>Have a screen reader active.</li> <li>Using that screen reader's Headings shortcut, open the Headings list. This moves focus outside the notebook and browser.</li> <li>Review and select heading to jump to.</li> <li>Keyboard focus and page scroll jumps to that heading.</li> </ol> <p>Because nbconvert for notebook to HTML properly captures Markdown cell headings as HTML headings, heading organization is technically available regardless of assistive tech or other settings. At the time of writing, though, there is no way for someone not using a screen reader to interact with the headings in the same way.</p> <p>Non-screen reader using participants frequently requested a table of contents to jump to major content areas in the same way that we saw screen reader using participants do. Only one screen reader using participants made the request for a table of contents, and even then it was mentioned as a personal preference over headings and not as a blocking issue.  No screen reader using participants expressed trouble jumping between content areas, and they all used heading navigation at some point.</p>"},{"location":"exports/user-tests/1-navigation/results/#by-preset-keys","title":"By preset keys","text":"<p>This navigation method is not reliant on any assitive tech or setting. Because keys are configurable, keyboard language/region can be configured, and physical keyboards can be different, there is a lot of variation on how users might have preset keys to navigate. During our sessions, we saw</p> <ol> <li>Have a keyboard with <code>end of page</code> and/or <code>top of page</code> keys configured.</li> <li>When participants knew or expected the goal would be near one end of a page, or simply closer to shorten navigation, they would use the relevant key to jump to a different area.</li> <li>Keyboard focus and page scroll jumps to that area.</li> <li>If needed, participants navigate the rest of the way to goal using an additional navigation method.</li> </ol> <p>Navigating by preset page navigation keys rarely took users exactly where they wanted to go on its own; it was used in combination with another method in any case where the goal was not singularly \"go to the bottom of the document\" (as in Task 4) or \"go to the top of the document\" (as in Task 5).</p> <p>This was used by participants who used browser zoom, or in one case a screen reader. Participants expect this to work at a browser level, so it is not tied specifically to the notebook.</p>"},{"location":"exports/user-tests/1-navigation/results/#by-zooming-and-skimming","title":"By zooming and skimming","text":"<p>This navigation method appeared exclusively with participants using browser zoom/magnification. They are similar, but have been broken into different steps to preserve the nuances of each.</p> <p>For a magnifier:</p> <ol> <li>Magnify the left side of the notebook. Amount needed may vary per participant, but the magnification stays at a consistent amount at first. Content does not reflow.</li> <li>Scroll along the left side of the notebook. What is visible depends on participants' magnifier settings; it could be that only a section of the browser window is visible on screen, that a single rectangular area is magnified while the rest of the window remains visible at 100% in the background, or things in between.</li> <li>Find an area that appears to be realted to the goal. Stop scrolling and adjust magnification as desired.</li> <li>Read the content while moving magnifier to the right to complete a line. </li> <li>At the end of each line, participants would move back to the left to start the next line unless they either found their goal or decided they would not complete their goal in the area.</li> <li>When they need to navigate again, participants would start at step one again.</li> </ol> <p>For browser zoom:</p> <ol> <li>Zoom browser to about 150\u2013250%. Content should reflow.</li> <li>Scroll along the left side of the notebook. Only the start and far right of each cell and output are visible in the window.</li> <li>Find an area that appears to be realted to the goal. Stop scrolling with it roughly centered on the page.</li> <li>Increase browser zoom to 300\u2013500%. Rescroll to desired point if necessary; the notebook did not hold scroll poisitons in the center so it was necessary for our tests, but participants noted this was not always their expectation.</li> <li>Read the content scrolling to the right to complete a line. </li> <li>At the end of each line, participants would scroll back to the left to start the next line unless they either found their goal or decided they would not complete their goal in the area.</li> <li>When they need to navigate again, participants would lower browser zoom to start at step one again.</li> </ol> <p>When navigating with this method, participants emphasized how important the far left content was to them, whether zooming in a way that caused the content to reflow or not. Working with notebooks in English means that we did not have the opportunity to test where participants would go to skim content in a right-to-left language or in non-document formatting, but it is safe to conclude that skimming and reading content in full are two different modes for people who navigate this way.</p> <p>Participants using this method also frequently brought up the utility of table of contents to hasten their navigation and lower the physical demands of reading as a result.</p>"},{"location":"exports/user-tests/1-navigation/results/#with-find-controls","title":"With <code>find</code> controls","text":"<p>This navigation method appeared exclusively with participants using screen readers. It only came up once throughout sessions and test hosts were only able to get limited information on it. it may not be entirely accurate. Reference this section with that in mind. </p> <ol> <li>From anywhere on the page or in the browser, open the screen reader's built in <code>find</code> type of controls.</li> <li>Input filtering criteria, review and select an option.</li> <li>Keyboard focus and page scroll jump directly to the selected option.</li> </ol> <p>This can be done with features like NVDA's Search for a word or a phrase or VoiceOver's rotor. It is similar to a browser's <code>find</code> features in that it filters the application content and allows users to navigate based on that.</p> <p>This method was used by participants most frequently when other navigation and skimming methods failed to help them complete a task. For example, it was common for participants to use several navigation methods when completing Task 2 since many expressed the author's name was not where they expected it to be (it was at the bottom of the document rather than the top). </p> <p>There was one instance where a screen reader participant used this navigation method right away, and that was because they had found the content needed to complete the task when skimming in a prior task and did not remember what heading it was under. They noted this was feasible because they remembered the content and knew what to filter for.</p>"},{"location":"exports/user-tests/1-navigation/results/#by-tabbing-through-interactive-areas","title":"By tabbing through interactive areas","text":"<p>This navigation method is not reliant on any assitive tech or setting. While this technique is possible to anyone regardless of OS, browser, or assistive tech, the only participants we saw using this were screen reader users.</p> <ol> <li>Use the <code>tab</code> key. The next interactive element in the focus order (ie. a link, a button, so on) will have keyboard focus.</li> <li>Use the <code>tab</code> key repeatedly until reaching the desired area. Focus order does loop, so one may jump from the bottom of the notebook to the top of the browser, for example.</li> </ol> <p>This navigation method was used infrequently, and it was used in combination with other navigation methods. Most frequently, tabbing was used as a fine-grain navigation once participants were in the general region they wanted to be. For example, a participant used a screen reader to jump to a content heading and then skip through cells via tabbing to skim for an area they were searching for.</p>"},{"location":"exports/user-tests/1-navigation/results/#common-feedback","title":"Common feedback","text":"<p>This is a list of the feedback that was most frequently or emphatically given. It is in no paritcular order.</p> <ul> <li>Requests for a table of contents. This was particularly important to participants not using screen readers.</li> <li>Notebooks need to be edited too, not just read. Participants that gave this feedback were aware of the scope of these tests and they wanted to emphasize that accessibility fixes also needed to happen for editable states.</li> <li>Notebook cells were of varying importance. How people want to understand and navigate the notebooks seemed to depend most on their expectations. Some participants talked in terms of cells or noted that they couldn't find non-visual cell sections (this was more common of participants who author notebooks, but not exclusive to them). Some participants talked in terms of content headings from the notebook cells. Some didn't mention either. This test did not allow us time to dive into why different participants had different mental models.</li> </ul>"},{"location":"exports/user-tests/1-navigation/results/#issues","title":"Issues","text":"<p>Bugs, issues, and other specific feedback or discussions from this round of tests can be found throughout the repository in issues. Listed below, they are</p> <ul> <li>Explore landmark options in rendered notebook</li> <li>Automatically add link to rendered notebook source</li> <li>Add a table of contents to rendered notebooks</li> <li>Page title and notebook title do not match</li> <li>\"Top of Page\" link in template footer bug</li> <li>Explore options for minimizing content on left side of rendered notebook</li> <li>Review/explore keyboard shortcuts in rendered notebooks</li> <li>Explore ARIA options in rendered notebook</li> <li>Code cells cut off content at high zoom in rendered notebook</li> <li>Vertical scroll jumping when adjusting browser zoom in rendered notebooks</li> <li>Table Reading with screenreaders</li> <li>Navigate to cells using keyboard commands</li> <li>Share Cell Content with screenreaders</li> <li>Move Metadata to the top</li> <li>Notebook Tutorial Link</li> <li>Markdown should be used only as intended</li> <li>Best Practice for Documenting Table Headers</li> </ul>"},{"location":"exports/user-tests/1-navigation/results/#questions-for-future-tests","title":"Questions for future tests","text":"<p>At the end of each session, we noted questions we wanted to further explore. This is the cumulative list.</p> <ul> <li>For a screen reader reading a code block in a Markdown cell, it read the content line by line instead of as a whole block. This was different than inline code styling or a code cell. Some participants expressed confuison. This would be good to text for mixed content types in a single cell, or even in the same line, perhaps.</li> <li>How easy is it to navigate in/out, and between certain content types (ie. tables were mentioned as \u201cif you\u2019re in a table its a pain to jump to another part of the page and then back to the same part of the table\u201d)?</li> <li>Should we explore UX for a table of contents? It is helpful for keeping context, but should be collapsible because of the space it can take up.</li> <li>Consider zooming in on content types as a task for future content-type tests.</li> <li>Should we let participants read the whole notebook first? Let them give first impressions and see how they decide to read the whole notebook.</li> </ul>"},{"location":"exports/user-tests/1-navigation/results/#what-we-would-do-differently-next-time","title":"What we would do differently next time","text":"<p>Reflecting on these sessions as hosts, for future tests we would like to </p> <ul> <li>Have a non-template notebook to work with. Multiple participants spent some of the tasks getting caught up figuring out why the notebook content switched topics often, expressed confusion that the notebook did not follow the narrative it expected when searching for information in multiple tasks, or found it difficult to summarize then notebook when asked in Task 5.</li> <li>Considering comparing some solutions side by side. As the first test, it is important to have a sense of the current state of nbconverted HTML notebooks but it doesn't always give us clarity on desired UX.</li> </ul>"},{"location":"exports/user-tests/1-navigation/test-script/","title":"Structure &amp; Navigation in Rendered Notebooks","text":""},{"location":"exports/user-tests/1-navigation/test-script/#introduction","title":"Introduction","text":"<p>Hi, I\u2019m _ I\u2018ll be running this meeting today. This is _, they are here to take notes. </p> <p>Thank you for taking the time to participate in this study. Before we begin, I\u2019ll give you a brief overview of how this will work. Throughout the hour, I will be reading from a script.</p> <p>This is completely voluntary. Please let me know at any point if you wish to stop participating.</p> <p>There\u2019s a few things I want to remind you about before we start.</p> <ul> <li>This session will be recorded, and we will be taking notes</li> <li>Recordings and notes will be saved online in a private folder that only our team members can access. Your private info is only available to the people working directly on this project</li> <li>Some anonymous information will be publicly available on our GitHub repo. https://github.com/Iota-School/notebooks-for-all </li> </ul> <p>During this study, we will be exploring a read only webpage that has been exported from a notebook. You can\u2019t edit it like a traditional notebook. I will be giving you a broad task to complete. I will set context for each task such as why you might be doing it and what you hope to achieve. Then I will ask some follow up questions.</p> <p>As you work through the task, please think out loud. Speak your thoughts as often as you can. Please tell us if something unexpected happens, if something works well, if you think something that could be improved, or if you\u2019re confused. We do not expect all the tasks to be easy to complete with assistive tech and we invite you to complain!</p> <p>It\u2019s important to know that we are not testing you, we are testing how Jupyter Notebooks behave with assistive tech. There are no correct or incorrect answers. You cannot do anything wrong. We are expecting them to break.</p> <p>If at any point you have questions, please don\u2019t hesitate to ask. Before we start, do you have any questions?</p>"},{"location":"exports/user-tests/1-navigation/test-script/#participant-introduction","title":"Participant Introduction","text":"<p>First, I want to ask a few things about you and your prior experience with Jupyter notebooks.</p> <ul> <li>What is the best way to send you links during this discussion? (in chat, spoken, email?)</li> <li>Would us sending messages in the chat to timestamp certain sections be disruptive for you? (Perhaps if you\u2019re using a screen reader)</li> <li>What operating system and browser are you using today? (We use this info to reproduce errors and support you if needed.) </li> <li>What assistive tech are you using during this session?</li> <li>How have you interacted with notebooks in the past? Tell me about the most common way you interact with a notebook?</li> </ul>"},{"location":"exports/user-tests/1-navigation/test-script/#notebook-tasks","title":"Notebook Tasks","text":"<p>Now that we know a little bit about you, I\u2019m going to ask you to complete some tasks using an uneditable form of a Jupyter notebook hosted by Space Telescope Science Institute (STScI). Once again, we are asking for honest feedback on the software\u2014please complain. </p> <p>First, can you please share your screen. When sharing, can you set it so that you either share the sound or share the text with a \u201cspeech viewer\u201d ex: In NVDA you can set it to hold the NVDA button (insert or caps) and hit N and then you get options, go down to tools, and select speech viewer.</p>"},{"location":"exports/user-tests/1-navigation/test-script/#task-1","title":"Task 1","text":"<p>Today we want to explore a platform that provides STScI notebooks in a non-editable form with all cell outputs shown. That means all the code has been run, we\u2019re just exploring the document without it changing, like reading a book.</p> <p>Now I\u2019d like you to open the style guide template notebook. I\u2019ll tell you how to do this part. </p> <ol> <li>Open your web browser of choice</li> <li>Paste this link: https://eteq.github.io/notebooks-for-all/14jun22_stsci_example_notebook.html https://bit.ly/3pgkFSl </li> </ol> <p>Can you tell me what the title of this notebook is? - Tutorial Title Y</p> <p>How easy or difficult was it for you to open the notebook?</p> <p>If you were to magically make opening this notebook easier, what would you change?</p>"},{"location":"exports/user-tests/1-navigation/test-script/#task-2","title":"Task 2","text":"<p>When you first open a notebook, there\u2019s some information to help you orient yourself.</p> <p>Can you tell me who the author of this notebook is? - Jessie Blogs, Archive Scientist Y</p> <p>What other [descriptive] information would help you orient yourself in the notebook?</p> <p>Are you satisfied with the organization of this information?</p> <p>Do you feel that you are missing any information? - What kinds of feedback do you search for to feel confident that you have all the information?</p>"},{"location":"exports/user-tests/1-navigation/test-script/#task-3","title":"Task 3","text":"<p>Next, we\u2019re going to explore the File Information Section.</p> <p>Show me how you would navigate to File Information. 1. Were they able to navigate to File Information Y 2. Notes on how they did it:</p> <p>There is a table in this section. Can you tell me the headers of the first two columns on the table? 1. obsID Y 2. Obs_collection Y</p> <p>How easy or difficult was it for you to navigate to and through the table?</p> <p>If you were able to magically make accessing this section easier, what would be the same? What would be different?</p>"},{"location":"exports/user-tests/1-navigation/test-script/#task-4","title":"Task 4","text":"<p>There\u2019s more information at the bottom of the notebook, so let\u2019s go there next. </p> <p>I\u2019d like you to navigate to the final cell of the notebook. 1. Were they able to navigate to the footer and/or citations? Y 2. Notes on how they did it:</p> <p>Can you tell me what information the footer (the section at the very bottom) includes? - Citations Y - Link to top of the page and stsci logo Y</p> <p>How easy or difficult was it for you to navigate and read the footer section?</p> <p>As a reader, what information do you expect to find in the notebook footer?</p>"},{"location":"exports/user-tests/1-navigation/test-script/#task-5","title":"Task 5","text":"<p>Now that you\u2019ve explored the notebook, I\u2019d like you to return to the top of the page</p> <p>Please return to the top cell. 1. Were they able to navigate back to the top? Y 2. Notes on how they did it:</p> <p>Please summarize the general topics this notebook covers for me as if I hadn\u2019t read it.</p> <p>Do you feel that you are able to access all the information in this notebook? What makes you feel that way?</p>"},{"location":"exports/user-tests/1-navigation/test-script/#follow-up-questions","title":"Follow Up Questions","text":"<p>Now that you\u2019ve explored the notebook, I\u2019d like to ask you to reflect on how that experience went and any other feedback you might have.</p> <p>Please complain \u2013 what was frustrating about navigating the Jupyter notebook with your assistive tech?</p> <p>Did the notebook provide enough (or the correct) information to help you know what to do and what decisions to make?</p> <p>Did the the notebook give you enough feedback to feel confident that you were completing the tasks successfully?</p> <p>Are you satisfied with the work-flow of moving through the notebook? (Navigation, number of steps, etc.) If so, what worked well? If not, what do you wish it did instead?</p> <p>Is there any information, options, or capability that is missing?</p> <p>Can you see yourself using Jupyter Notebooks in a non-editable form? (Y) (N)  Why or why not?</p> <p>Do you have any other impressions or feedback that you would like to share?</p> <p>Do you have any questions for me?</p> <p>Thank you so much for participating!</p>"},{"location":"exports/user-tests/1-navigation/test-script/#interview-debrief","title":"Interview Debrief","text":"<p>After each ethnographic interview you complete, take a few minutes to perform an interview debrief while the session is fresh in your mind. This ensures that key learnings and observations are not lost in the scramble of many interviews or long timelines.</p> <p>What are our action items based on this feedback?</p> <p>Any more details on issues we already discovered?</p> <p>Are there any new questions I should explore in a further script?</p> <p>What are some key quotes that I heard?</p>"},{"location":"exports/user-tests/2-content/Test-script/","title":"Content Types in Rendered Notebooks","text":""},{"location":"exports/user-tests/2-content/Test-script/#introduction","title":"Introduction","text":"<p>Hi, I\u2019m _ I\u2018ll be running this meeting today. This is _, they are here to take notes. </p> <p>Thank you for taking the time to participate in this study. A few things I want to remind you about before we start: - This is completely voluntary. Please let me know at any point if you wish to stop participating. - You can ask questions at any time. - We are not testing you, we are testing how Jupyter Notebooks behave with assistive tech. There are no correct or incorrect answers. - You signed a recording consent form before this session. Would you like me to go over what we are recording and where it will be stored as a reminder?</p>"},{"location":"exports/user-tests/2-content/Test-script/#participant-introduction","title":"Participant Introduction","text":"<p>First, I want to ask a few things about you. If we\u2019ve met before, I\u2019d still like to confirm whether or not things have changed.</p> <p>What operating system and browser are you using today? (We use this info to reproduce errors and support you if needed.)</p> <p>What assistive tech are you using during this session?</p> <p>During this study, we will be exploring a read only webpage that has been exported from a notebook focused on astronomy data analysis hosted by Space Telescope Science Institute.  That means all the code has been run. You can\u2019t edit it like a traditional notebook. I will be giving you broad tasks to complete and ask follow up questions.</p> <p>Throughout the session, please think out loud. Tell us if something unexpected happens, if something works well, if you think something could be improved, or if you\u2019re confused. We do not expect all the tasks to be easy to complete with assistive tech and we invite you to complain!</p> <p>Before we start, do you have any questions?</p>"},{"location":"exports/user-tests/2-content/Test-script/#notebook-tasks","title":"Notebook Tasks","text":"<p>First, can you please share your screen. If using a screen reader, can you set it so that you share the sound or share the text with a \u201cspeech viewer\u201d ex: In NVDA you can set it to hold the NVDA button (insert or caps) and hit N and then you get options, go down to tools, and select speech viewer.</p>"},{"location":"exports/user-tests/2-content/Test-script/#task-1-opening-the-notebook","title":"Task 1 - Opening the notebook","text":"<p>Now I\u2019d like you to open today\u2019s notebook. I\u2019ll tell you how to do this part. </p> <ol> <li>Open your web browser of choice</li> <li>Paste this link: https://eteq.github.io/notebooks-for-all/Imaging_Sky_Background_Estimation.html</li> <li>tinyurl.com/37pncd8n  </li> </ol> <p>Can you tell me the title of this notebook?</p> <ul> <li>Complex 2D Background Y or Imaging_Sky_Background_Estimation Y</li> </ul>"},{"location":"exports/user-tests/2-content/Test-script/#task-2-markdown-cell-basics","title":"Task 2 - Markdown cell basics","text":"<p>First, we\u2019re going to start by exploring the foundational units of a notebook: cells. </p> <p>Are you familiar with notebook cells? If so, can you briefly describe them for me?</p> <p>Cells can hold different content types. When they are run, which they have been on this webpage, they may show additional information. We want to understand how you identify and interact with these different types of content.</p> <p>Start by navigating to the beginning of the notebook. Take a few moments to read down to the Imports heading.</p> <p>What type of content did you find in the cell?</p> <ul> <li>Markdown Y  | Writing Y | HTML Y</li> </ul> <p>This is a Markdown cell. In an uneditable version of a notebook like this one, it is run and appears as text.</p> <p>Please tell me about the process you used when reading that section. Were you able to do everything you expected? Did anything surprise you?</p>"},{"location":"exports/user-tests/2-content/Test-script/#task-3-code-cell-basics","title":"Task 3 - Code cell basics","text":"<p>Please return to the end of the Imports section if you navigated elsewhere. You should be at the end of an unordered (bulleted) list. We\u2019re going to explore the section directly below the unordered (bulleted) list.</p> <p>What type of content did you find in the cell?</p> <ul> <li>Code Y </li> </ul> <p>This is a code cell. In this case, it is importing libraries for use later in the notebook. When code cells are run, they are numbered in the order they are run. They also will be labeled as an input (the code the author writes) and an output (the result of running the input). Sometimes additional messages appear when a code cell is run. </p> <p>Can you tell me what number this cell was run as and if it is an input or output?? Cells are labeled visually, and we want to confirm you have access to that label.</p> <ul> <li>In [1]: Y</li> </ul> <p>How did you find your experience of reading that section? Were you able to do everything you expected? Did anything surprise you?</p> <p>This cell has an additional area of text below the input code block. It is labeled visually with an alternate background color, but may not be labeled non-visually. Please tell me what you think it is. What makes you think that?</p> <ul> <li>This section is an error or warning message. Y</li> </ul>"},{"location":"exports/user-tests/2-content/Test-script/#task-4-repeat-task-3-in-alternate-notebook","title":"Task 4 - repeat Task 3 in alternate notebook","text":"<p>Now I\u2019d like you to open an alternate version of the notebook we just reviewed. This notebook has the same content, but different structure. We\u2019ll be asking you to complete some of the same tasks, so don\u2019t worry if you feel like you are giving the same answers. We\u2019re curious to hear you compare the experiences and if you have any preferences.</p> <ol> <li>Open your web browser of choice</li> <li>Paste this link: https://iota-school.github.io/notebooks-for-all/user-tests/2-imnotsurewhat/Imaging_Sky_Background_Estimation.html.html5.html</li> <li>tinyurl.com/4m9cxeyu </li> </ol> <p>Can you read me the title of this notebook?</p> <ul> <li>Complex 2D Background Y</li> </ul> <p>Please navigate to the start of the Imports section and skim-read to the next cell.</p> <p>What type of content did you find in this section?</p> <ul> <li>Markdown (Writing, HTML) and Code Y </li> </ul> <p>**Can you find the number this cell was run as and if this was an input or output? **</p> <ul> <li>In [1]: Y</li> </ul> <p>How was the process of navigating to this cell compared to the experience from the last web page? Do you have a preference?</p>"},{"location":"exports/user-tests/2-content/Test-script/#task-5-single-chart-png","title":"Task 5 - Single chart PNG","text":"<p>Now that we\u2019ve covered the basics of cells, we\u2019re going to explore other types of outputs in this notebook.</p> <p>Please navigate to Out [7]:, or the output of code cell 7. </p> <ul> <li>More support needed? Y<ul> <li>In the \u201cCreate the nasty sky background\u201d heading level 3.</li> <li>Above the \u201cLook at it with noise added and then smoothed a bit\u201d heading level 3.</li> <li>Below the table.</li> </ul> </li> </ul> <p>Please tell me what kind of output you think this is. What makes you think that?</p> <ul> <li>This is a chart/graph/plot. Y</li> </ul> <p>What can you tell me about this chart? What information does this output give you? Does this meet your expectations?</p> <p>**Please read us the tics on the Y-axis **</p> <ul> <li>0, 200, 400, 600, 800 Y</li> </ul> <p>What would you do if you needed to find more information about this output?</p> <p>Please navigate to In [7]:, or the input of code cell 7.</p> <ul> <li>More support needed? Y<ul> <li>This is directly above where we ended our last question.</li> </ul> </li> </ul> <p>How did you identify the cell\u2019s input? What information told you these sections were related? What do you wish it told you?</p> <p>Please navigate to the \u201cLook at it with noise added and then smoothed a bit\u201d section (heading level 3). Go to the first code cell in this section. Facilitator note: this is cell In [13]:. Please navigate to this code cell\u2019s output.</p> <p>Is there anything different from this image output compared to the previous png you observed? How can you tell?</p>"},{"location":"exports/user-tests/2-content/Test-script/#task-6-iframe-video-and-errors","title":"Task 6 - Iframe, video, and errors","text":"<p>Please navigate to the video on this page. - Can they easily search for a video without hint Y - Hint: this is cell Out [19]: This is also in the Video1: section (heading level 2).</p> <p>Please press the play button to activate the video. You may stop/pause it immediately after.</p> <p>Walk me through what you needed to do to complete this task.</p> <p>What can you tell me about this video output? What information does this output give you? Does this meet your expectations?</p> <p>What would you do if you needed to find more information about this video output?</p> <p>Please navigate to the input code cell for this video. - Facilitator note: this is cell In [19]:</p> <p>Tell me what you find in the input code cell. Is there any information here you wish was in the output as well?</p>"},{"location":"exports/user-tests/2-content/Test-script/#task-7-different-single-chart-svg","title":"Task 7 - Different single chart SVG","text":"<p>Please navigate back to your original browser tab.</p> <p>Please navigate to Out [54]:, or the output of code cell 54.  - If more support is needed to navigate, this is in the \u201cRoutines to facilitate looking at the RMS of the residual background as a function of scale\u201d heading level 2. - This chart is the very bottom of the page (excluding footer logo).</p> <p>Please tell me what kind of output you think this is. What makes you think that?</p> <ul> <li>This is a chart/graph/plot. Y</li> <li>What type of chart?</li> </ul> <p>Please read us the tics on the Y-axis </p> <ul> <li>BKG2, BKG3, BKG4, BKG5, Perfect Y</li> </ul> <p>What can you tell me about this chart? What information does this output give you? Does this meet your expectations?</p> <p>This is a different kind of chart than the prior ones we explored. Are you able to tell what types of charts these are?</p> <p>What information would better help you understand the charts?</p>"},{"location":"exports/user-tests/2-content/Test-script/#follow-up-questions","title":"Follow Up Questions","text":"<p>Now that you\u2019ve explored the notebook, I\u2019d like to ask you to reflect on how that experience went and any other feedback you might have.</p> <p>Was there a difference in how you experienced the first version of the notebook we linked you compared with the second one?</p> <p>Please complain \u2013 was there anything frustrating about reading content in the Jupyter notebook with your assistive tech?</p> <p>Did you feel confident that you were given all the information available?</p> <p>If you felt information was missing, how would you have preferred that information to be communicated to you? </p> <p>Do you have any other impressions or feedback that you would like to share?</p> <p>Do you have any questions for me?</p>"},{"location":"exports/user-tests/2-content/Test-script/#interview-debrief","title":"Interview Debrief","text":"<p>After each ethnographic interview you complete, take a few minutes to perform an interview debrief while the session is fresh in your mind. This ensures that key learnings and observations are not lost in the scramble of many interviews or long timelines.</p> <p>What are our action items based on this feedback?</p> <p>Any more details on issues we already discovered?</p> <p>Are there any new questions I should explore in a further script?</p> <p>What are some key quotes that I heard?</p>"},{"location":"exports/user-tests/2-content/results/","title":"Results: Content Access in Rendered Notebooks","text":"<p>These results are from user interviews conducted from November 2022 to January 2023 with the content types test script on the STScI Imaging Sky Background Estimation example notebook.</p>"},{"location":"exports/user-tests/2-content/results/#what-we-tested","title":"What we tested","text":"<p>Operating systems: Mac OS Monterey, Windows 10, Windows 11</p> <p>Browsers: Chrome, Firefox, Safari, Edge</p> <p>Assistive tech: JAWS (screen reader), NVDA (screen reader), VoiceOver (screen reader), Mantis (braille reader), Mac OS Zoom (built-in screen magnifier), color inversion, built-in browser zoom controls, built-in large cursor and pointer settings</p> <p>Interface: Browser and notebook in HTML form hosted via GitHub pages</p> <p>Sample size: 7 participants</p> <p>Method: Combination of qualitative usability testing and user interviews</p>"},{"location":"exports/user-tests/2-content/results/#content-types","title":"Content types","text":"<p>This round of tests emphasized a notebook with a range of content types and tasks designed to have participants engage with them. The following sections describe the experience and feedback of participants sorted by content. They are not listed in any particular order.</p>"},{"location":"exports/user-tests/2-content/results/#cells","title":"Cells","text":""},{"location":"exports/user-tests/2-content/results/#markdown-cells","title":"Markdown cells","text":"<p>Markdown cells were an approachable format for all participants. Whether they are relying on the visuals of rich text rendering or the reliable HTML underpinning it, the consensus is that Markdown content read as expected and was easy to work with because of it. </p> <p>Cells continue to have unclear divisions, a fact made more unclear by Markdown cells\u2019 lack of execution number or any other visual division. For the most part, this did not inhibit participants. It only became a factor when participants were trying to navigate by cell.</p> <p>Participants did not display any major issues working with Markdown content. Some assistive tech, like the JAWS screen readers, handled technical language like inline code in a way that participants found confusing. Because other reading methods, like non-JAWS screen readers and the Mantis Braille reader, handled it in a way that did not confuse their participants, we can only advise that notebook authors be aware; this does not appear to be an issue the notebook itself is responsible for.</p>"},{"location":"exports/user-tests/2-content/results/#code-cells","title":"Code cells","text":"<p>Like other cells, code cells continue to have unclear divisions, though they do have the boon of execution numbers, borders, and shading that provide visual hints. Formally, though, assistive tech did not present them as separate sections. Only participants familiar with notebooks searched for these as a means of determining cell divisions. Because code often has syntax that is distinctive from non-code, the most code-savy participants recognized code cells exclusively because they were familiar with the different words/commands/content. For the most part, this did not inhibit participants. It only became a factor when participants were trying to navigate by cell.</p> <p>With structural changes made to the notebook before this test, screen readers did pick up cells like code cells as an HTML <code>article</code>. Some screen readers allow users to navigate by article. The issue was that all code cells were referred to as \u201carticle\u201d alone without any unique descriptions (including no mention of execution number) or mention of it being code at all. While it was good to find the change surfacing, it was not yet useful to users.</p> <p>The way participants read code cells varied widely based on their set up (ie. operating systems, settings, assistive tech) and personal preference. Options included</p> <ul> <li>Reading code as any other default text, meaning word by word.<ul> <li>This was done by participants using their vision and some screen reader users. With this option, it was possible to miss a switch between Markdown and other code cells, though most participants noticed within the first few seconds because their contents are typically distinct. </li> <li>Those using vision mentioned that reading code requires more active reading than recognizing word shapes because code often uses made-up or conjoined words rather than familiar ones; modifying font/font choice becomes critical in allowing participants to manage the strain active reading puts on them.</li> </ul> </li> <li>Reading code character by character.<ul> <li>This was mentioned by participants using screen readers, though it seems probable that participants using their vision would do the same to identify unfamiliar words.</li> <li>Some participants switch to this screen reader setting when they code, others assign this as a preset to a list of applications, and others manually adjust this setting when they feel reading word by word is not serving their needs.</li> </ul> </li> <li>Reading white space in addition to another setting.<ul> <li>This means the white space character\u2019s ( space, tab, etc.) name is read with each repetition.</li> <li>This was only mentioned by screen reader users.</li> <li>Participants that used this method said it can help them identify sections of code outright because the use and frequency of white space is so different than in non-code. This can also help them gain relevant information in code languages where white space is meaningful.</li> </ul> </li> <li>Manually initiating a preset workflow to copy whole blocks of code\u2014like notebook code cells\u2014and paste them into another application that is more accessible to them.<ul> <li>Here they may read, edit, and run code line by line.</li> <li>We did not discuss what they would do if they needed to get code back in the notebook.</li> <li>This was mentioned by the most code-savvy screen reader users, but the workflow is not done on a screen reader itself.</li> </ul> </li> <li>Reading in chunks by a set number of characters, including white space characters.<ul> <li>These chunks do not split at words or other divisions; it is purely by number of characters.</li> <li>This was mentioned by Braille reader users\u2014the number of characters per chunk is determined by the Braille reader\u2019s display.</li> </ul> </li> </ul>"},{"location":"exports/user-tests/2-content/results/#output-errors-and-warnings","title":"Output errors and warnings","text":"<p>Error and warning information appears as additional cell outputs in Jupyter notebooks, they are grouped separately from an intended run cell output. For brevity, we\u2019ll refer to both errors and warnings simply as errors for the rest of this section. </p> <p>Finding and identifying errors was a challenge for most participants and extremely difficult for those not using vision. Error information is almost entirely visual. Other than the error text itself, errors are differianted with padding: a visual break between the section above and below, and low-contrast color coding: a light red or orange background. The color coding was not discernable for those using color inversion or screenreaders. Errors syntax melds directly with the outputs above and below them, so they can be obscured by having a standard text output and error text back to back. Participants using vision, like those with screen magnifiers or high zoom, usually noticed the visual break and thus found errors quickly. Participants with screen readers consistently heard the error message read, but since there was no syntactic break between the error and the other code cells, it was frequently missed. Screen readers read them as one long paragraph rather than as separate text sections. </p> <p>Most participants recognized errors because of their familiarity with the message; they were forced to rely on the content rather than organizational structure, a much slower reading experience. Because words like \u201cerror\u201d or \u201cwarning\u201d often appear in the middle of the message rather than the start, this meant a delay in recognition. This recognition relied on user expertise and prior familiarity with the notebook itself, biasing readability against users newer to the field and those with different cognitive abilities. Some participants with more notebook experience used their knowledge of notebook layout, errors appear after the cell, to search in depth.</p> <p>This comment from a screen reader user summarizes the inaccessible error design's impact: my sighted colleagues can find errors and similar unexpected feedback quickly, but I have to be vigilant and intentionally search to make sure I receive necessary and important information.</p> <p>Participants handled the challenge in a few ways:</p> <ul> <li>Reading all text very carefully and stopping over any area they recognized a keyword (\u201cerror,\u201d \u201cwarning,\u201d or a specific name of an error if they knew the code language).</li> <li>Rereading cells and their outputs when they noticed elsewhere that something was amiss in the notebook.</li> <li>Using the browser find tool to search the notebook\u2019s contents, usually with keywords like \u201cerror\u201d or \u201cwarning.\u201d They navigated using the keyboard.</li> <li>Using a screen reader\u2019s feature to read aloud the background color of a region. This only helps when users first suspect an area is worth investigating. It is not a passive feature and must be intentionally activated by the user.</li> </ul>"},{"location":"exports/user-tests/2-content/results/#images","title":"Images","text":""},{"location":"exports/user-tests/2-content/results/#png-images","title":"PNG images","text":"<p>PNG images proved an obstacle for most participants. Different notebook authoring choices can be employed to fix many of these issues.</p> <p>The primary issue with images is that they provide information in an inflexible way. Participants using vision can magnify or zoom into an image (though browser zoom does not apply), but that is the only control participants were able to exercise over images. If these same participants cannot get information because the image has areas that are too low contrast, there is nothing they can do; any theme changes or color inversions they may apply elsewhere will not apply to an image. Because of this, low contrast is an extreme blocker for participants to complete tasks related to images.</p> <p>This notebook provides absolutely no support for participants not using vision. Because the images in this notebook had no alt text or image descriptions, were not described in surrounding context, and provided no ways to access the source information, screen reader and Braille reader users could not complete any image-related tasks in these tests. At most, participants would take a guess based on the image file\u2019s name\u2014the only information their assistive tech could access. </p> <p>To manage this poor experience, all participants:</p> <ul> <li>Would read before and after the image to try and glean the image\u2019s surrounding context.</li> <li>Would search for any links that might send them to an image source, related data, or provide other context.</li> </ul> <p>Participants using their vision also:</p> <ul> <li>Tried to magnify or zoom in on the image. Which methods of magnification or zoom they tired first depended on their personal preference and amount of zoom needed. Remember that browser zoom does not zoom the image in a rendered notebook.</li> <li>Would adjust their display settings, like using a high contrast mode or inverting colors. These did not successfully impact images.</li> </ul> <p>Participants using screen readers or Braille readers also:</p> <ul> <li>Would explore more aggressively when searching for image information and often noted there was no other recourse. They were locked out from the information in an undescribed image.</li> </ul>"},{"location":"exports/user-tests/2-content/results/#svg-images","title":"SVG images","text":"<p>With a default\u2014meaning not manually tagged or otherwise modified\u2014SVG image, participants noted no differences between the experience of the PNG plots versus the SVG plots in this notebook.</p>"},{"location":"exports/user-tests/2-content/results/#chartvisualization-feedback","title":"Chart/visualization feedback","text":"<p>In the notebook used for these tests, a majority of the images were charts or other kinds of visualizations. We received the following feedback on charts used during the test:</p> <ul> <li>Missing chart information is confusing at best and misleading at worst. Participants often struggled to make sense of what the charts were trying to explain to them because several were missing titles or axis labels. Not including these fundamental aspects has direct negative impact on readability.</li> <li>In most cases, summarizing or including a description of the information a chart is meant to convey can help participants dive into the information faster and more deeply. If done as a text description, this may also serve as support for screen reader and Braille users.</li> <li>Default styling of plots were some of the biggest contrast obstacles for participants engaging with the notebook visually. The default styling often had low contrast color choices representing the data, thin and low opacity lines for trend-lines and axes, and small and thin text. Because they are images, colors and lines and text are not customizable or restylable even for participants who wanted to try editing them using developer tools. </li> <li>Use grid lines in charts. When using high magnification, zoom, or otherwise handling a limited field of vision, participants using their vision often could not see both the axes and the data at the same time. Without gridlines, they had nothing to follow to orient that data point in relation to the axes.</li> <li>Including the tables of data used to create complex charts was an unpleasant but reliable way of accessing the information in the plot. This was the same answer whether tables were included in the notebook already or if users would have to generate one themselves. For participants more experienced with data analysis, having access to the source notebook and data was consistently preferable.</li> </ul>"},{"location":"exports/user-tests/2-content/results/#videos","title":"Videos","text":"<p>The <code>iframe</code> linked video in our test notebook was mostly usable by participants who completed tasks relating to the video more often than not. When asked to reflect on the experience participants credited the fact that the video and its interface were easier to use because it followed video patterns they expected and experienced elsewhere on the internet. </p> <p>Video feedback included:</p> <ul> <li>The video did not immediately appear to be a video.<ul> <li>Participants using vision mistook it for an image because the video player around it does not appear until a user interacts with it.</li> <li>Participants using screen readers commented that the video did not have any kind of labeling that told them it was a video; they figured it out when they found the familiar play button (though it did not tell them whether that would be audio or video). This is likely a result of the <code>iframe</code> and no additional labeling.</li> </ul> </li> <li>Participants would like closed caption and/or transcription options.</li> <li>The area the video took up was unclear.<ul> <li>This became an issue for participants using their vision when they were trying to figure out where they could click to pause and play the video.</li> <li>When magnified or zoomed in, it could also become difficult to tell which parts were video and which parts were notebook background. </li> </ul> </li> <li>Because the video player does not appear around the video until the video is played, its controls are unclear. The initial play button that appears as an overlay of the video thumbnail does not seem to be labeled as a play button; screen reader users were guessing when they activated the button.</li> </ul>"},{"location":"exports/user-tests/2-content/results/#content-types-not-covered","title":"Content types not covered","text":"<p>While we aimed to cover a breadth of commonly used content types in these tests, we could not cover everything that could possibly be put in a notebook\u2019s cells. For this round of tests, we intentionally did not focus on</p> <ul> <li>External links</li> <li>Tables</li> <li>Iframes</li> <li>Interactive Widgets</li> </ul> <p>The decision to not focus these was motivated by constraints like the length of a testing session (one hour per participant), the STScI notebooks available to us, the content types most commonly found in public-facing STScI notebooks, prioritizing content types that had received no feedback in prior sessions, and interactivity limits in a rendered notebook.</p>"},{"location":"exports/user-tests/2-content/results/#navigation-more-feedback","title":"Navigation: more feedback","text":"<p>Our prior tests were centered around notebook structure and subsequent navigation. Because we addressed some feedback between test sessions and exploring content types does first require navigating to that content, we found further feedback on navigation in this round.</p> <p>Two visibly identical notebooks with different underlying structures were used during these tests. Feedback is sorted by notebook.</p>"},{"location":"exports/user-tests/2-content/results/#labeled-cells-notebook-structure-1","title":"Labeled cells: notebook structure 1","text":"<p>This notebook structure directly addressed some of the feedback from previous rounds of tests. Headings automatically became links. Execution numbers became their own grouping rather than a portion of the tags encapsulating all content for that cell. <code>div</code> tags were removed wherever possible. Participants reported the following.</p> <ul> <li>This version of the notebook provided some improvement in navigation for most participants. Having headings as links allowed non-screen reader users to take advantage of headings. Having headings as links added headings to the list of interactive elements on the page and made them appear with the tab key. This offered an additional way for screen reader users and keyboard navigators to explore the notebook structure.</li> <li>Requests for a table of contents continued.</li> <li>This version received less feedback overall; it was noted as relatively similar to previous tests.</li> </ul>"},{"location":"exports/user-tests/2-content/results/#tabable-cells-notebook-structure-2","title":"Tabable cells: notebook structure 2","text":"<p>This notebook structure labeled each cell as an <code>article</code> and leveraged other HTML content categories to provide a more standard HTML structure for assistive tech to hook into. It also made adjustments to low-contrast areas. Participants reported the following.</p> <ul> <li>This version of the notebook received consistently neutral to unfavorable feedback.</li> <li>Non-screen reader users responded neutrally and were unimpacted.<ul> <li>Participants using their vision did give positive feedback on increased color contrast in the execution number and code comments.</li> </ul> </li> <li>Screen reader users were most impacted by these changes and gave negative feedback.<ul> <li>Participants using a screen reader generally needed to be prompted to discover the notebook\u2019s structural changes; the methods it supported them using were not the ways these participants were interested in interacting.</li> <li>Participants successfully used the tab key to navigate through cells (to \u201ctab through cells\u201d). However, populating the tab list with every cell on top of all interactive areas in the notebook created an \u201coverhead of tabs.\u201d One participant described it as \u201cyou don\u2019t know where you are going or what you are looking for. Could be five tabs. Could be fifty\u201d before they can complete the task. This was not considered a positive change.</li> <li>To clarify, the tab key is, by default, a coarse navigation tool that allows users to jump to and from areas where they can perform some kind of interaction. Without this change in notebook structure, which already included all inline links, headings (because they are also links), the video play button, and all browser-level navigation. Making cells tabable adds an additional type of content to filter through in this list, and adds another fifty-plus items to the list in this over-fifty cell notebook.</li> </ul> </li> </ul>"},{"location":"exports/user-tests/2-content/results/#additional-notes","title":"Additional notes","text":"<p>We found larger UX patterns worth noting. They are listed in no particular order:</p> <ul> <li>There were issues searching and navigating by content type (ie. cell, image, video, so on), but there was a high rate of eventual success. Most tasks were completed by most participants.</li> <li>A common sentiment in tests: \u201cannoying but normal.\u201d Participants expressing this sentiment would first encounter an obstacle they knew how to overcome. They would then report that this obstacle was an everyday occurrence for them across the internet and that the notebook was behaving within the current standard for that user experience. Unfortunately, this was one of the most positive types of feedback we received. It tells us we have a lot of room to grow in making enjoyable and equitable user experiences in both Jupyter and in wider digital spaces.</li> <li>Unlike in the first set of tests for navigation, participants were more likely to miss information or not be able to access it at all. Interestingly, very few participants expressed that they noticed they were missing information; most remained confident they had access to the whole notebook. The few who did observe that they could not access information knew because they found familiar failures\u2014especially images lacking descriptions. </li> <li>Many issues and fixes (requested by participants or found in review) are what might be considered accessibility \u201cbasics.\u201d Alt text/image descriptions, labeling, and contrast issues came up frequently. These are very fixable issues, and they need to be done both in the interface and when authoring individual notebook files.</li> <li>Participants who are more comfortable and/or familiar with Jupyter notebooks expressed more interest in working with the source notebook when encountering obstacles or when trying to find information that wasn\u2019t immediately findable. Filtering through the non-editable version of the notebook was comparatively not worth the effort.</li> <li>Text-based content regularly gave participants fewer issues when compared to non-text content like images or videos. While no content type was without issue, inaccessible images and videos were more likely to block participants completely.</li> <li>Participants using screen magnifiers are especially impacted by the lack of maximum width for notebooks in this form. Because magnifying limits how much information fits on a screen and horizontal scrolling is typically more awkward than vertical scrolling, the full-window line length of notebook content came up as a serious pain point and contributor to fatigue. It also increased the risk of screen magnifier users missing information, especially on the right-hand side (for a notebook in English, a left-to-right language).</li> <li>Some participants would complete or describe completing tasks using an ability that fatigued or even hurt them. For example, participants with low vision strained to use their vision to complete a task that their assistive tech was unable to work with (due to poor infrastructure or tagging on Jupyter\u2019s part). Yet another way that inaccessibility harms people who are determined to work in fields that rely heavily on notebooks.</li> <li>Jupyter notebooks often bring together many types of content, and this content can bring its own accessibility issues with it. Notebooks have the capacity to inherit accessibility problems from everything that makes them up\u2014from Jupyter-maintained tools to any other package. For these tests, we ran into issues like lack of image description support for plotting packages, lack of labeling in the embedded video player and its buttons, and low contrast syntax highlighting themes. On the Jupyter side, we can also make choices about what packages to support or how we handle these inaccessible defaults. Notebooks can surface inaccessibility from anywhere.</li> <li>Authors will continue to have a large amount of power to determine the accessibility of an individual document. This is part of why we are drafting authoring recommendations.</li> <li>Participants search for familiarity to anchor their experience. What was familiar to each participant varied depending on their field of expertise, accessibility accommodations used, what other software they were familiar with, and Jupyter notebook experience specifically. Examples include:<ul> <li>Participants who are familiar with Jupyter notebooks would more often talk about cells and try and find ways to distinguish between them. They also were the only participants who called out insufficient divisions and information to find cells.</li> <li>Participants using screen readers were more likely to expect content headings to be more robust. These participants were also more likely to explain their mental model of cells (or other divides) in the notebook by the idea of headings. </li> <li>Participants used to working with editable versions of notebooks or other source code forms were more likely to compare behaviors to an editable document and asked to have those experiences carry over. For example, some participants wanted to be able to navigate content by editable versus non-editable areas to tell the difference between cell inputs and outputs.</li> <li>Error and warning outputs\u2014an (often unexpected) cell output that reports to users when something in the code run is not functioning as expected\u2014were only findable because some participants knew to expect one. Many participants missed the text-only transition to an error message in the test notebook because it had no other indicators. As it was a common error, some participants clocked into it immediately and without host support, but they reported it was only because they had heard that exact sentence many times before.</li> </ul> </li> </ul>"},{"location":"exports/user-tests/2-content/results/#issues","title":"Issues","text":"<p>Bugs, issues, and other specific feedback or discussions from this round of tests can be found throughout the repository in issues. Listed below, they are * Additional comment on Page title and notebook title do not match * Additional comment on Explore landmark options in rendered notebook * Syntax highlighting feedback * Improve color contrast in rendered notebook * Improve errow/warning cell outputs * Improve video output experiences * Cell navigation feedback</p>"},{"location":"exports/user-tests/2-content/results/#questions-for-future-tests","title":"Questions for future tests","text":"<p>Based on each session and the conversations we had with the development team, we\u2019ve come up with a list of questions we\u2019d like to explore further.</p> <ul> <li>What is the most expected or desirable cell navigation experience? Could there be more than one? What are the different possibilities and how could we best compare them?</li> <li>Should keyboard shortcuts be a part of the rendered notebook experience? Or should we make better use of the browser defaults?</li> <li>How might we increase the discoverability of varied navigation methods in the notebook?</li> <li>What are the limits on cell output types and/or content in notebooks converted to HTML? How does the default accessibility on each type stack up?</li> </ul>"},{"location":"exports/user-tests/2-content/results/#what-we-would-do-differently-next-time","title":"What we would do differently next time","text":"<p>Reflecting on these sessions as hosts, for future tests we would like to * Explore a standard notebook with the STScI computer science channels to further discuss ideas for landmarks or other structural options in notebooks. * Experiment with asynchronous test sessions for shorter tasks. This might enable an increased number of and more iterative A/B testing in the long term.</p>"},{"location":"exports/user-tests/2-content/results/","title":"Results: Content Access in Rendered Notebooks","text":"<p>These results are from user interviews conducted from November 2022 to January 2023 with the content types test script on the STScI Imaging Sky Background Estimation example notebook.</p>"},{"location":"exports/user-tests/2-content/results/#what-we-tested","title":"What we tested","text":"<p>Operating systems: Mac OS Monterey, Windows 10, Windows 11</p> <p>Browsers: Chrome, Firefox, Safari, Edge</p> <p>Assistive tech: JAWS (screen reader), NVDA (screen reader), VoiceOver (screen reader), Mantis (braille reader), Mac OS Zoom (built-in screen magnifier), color inversion, built-in browser zoom controls, built-in large cursor and pointer settings</p> <p>Interface: Browser and notebook in HTML form hosted via GitHub pages</p> <p>Sample size: 7 participants</p> <p>Method: Combination of qualitative usability testing and user interviews</p>"},{"location":"exports/user-tests/2-content/results/cell-contents/","title":"Content types","text":"<p>This round of tests emphasized a notebook with a range of content types and tasks designed to have participants engage with them. The following sections describe the experience and feedback of participants sorted by content. They are not listed in any particular order.</p>"},{"location":"exports/user-tests/2-content/results/cell-contents/#cells","title":"Cells","text":""},{"location":"exports/user-tests/2-content/results/cell-contents/#markdown-cells","title":"Markdown cells","text":"<p>Markdown cells were an approachable format for all participants. Whether they are relying on the visuals of rich text rendering or the reliable HTML underpinning it, the consensus is that Markdown content read as expected and was easy to work with because of it. </p> <p>Cells continue to have unclear divisions, a fact made more unclear by Markdown cells\u2019 lack of execution number or any other visual division. For the most part, this did not inhibit participants. It only became a factor when participants were trying to navigate by cell.</p> <p>Participants did not display any major issues working with Markdown content. Some assistive tech, like the JAWS screen readers, handled technical language like inline code in a way that participants found confusing. Because other reading methods, like non-JAWS screen readers and the Mantis Braille reader, handled it in a way that did not confuse their participants, we can only advise that notebook authors be aware; this does not appear to be an issue the notebook itself is responsible for.</p>"},{"location":"exports/user-tests/2-content/results/cell-contents/#code-cells","title":"Code cells","text":"<p>Like other cells, code cells continue to have unclear divisions, though they do have the boon of execution numbers, borders, and shading that provide visual hints. Formally, though, assistive tech did not present them as separate sections. Only participants familiar with notebooks searched for these as a means of determining cell divisions. Because code often has syntax that is distinctive from non-code, the most code-savy participants recognized code cells exclusively because they were familiar with the different words/commands/content. For the most part, this did not inhibit participants. It only became a factor when participants were trying to navigate by cell.</p> <p>With structural changes made to the notebook before this test, screen readers did pick up cells like code cells as an HTML <code>article</code>. Some screen readers allow users to navigate by article. The issue was that all code cells were referred to as \u201carticle\u201d alone without any unique descriptions (including no mention of execution number) or mention of it being code at all. While it was good to find the change surfacing, it was not yet useful to users.</p> <p>The way participants read code cells varied widely based on their set up (ie. operating systems, settings, assistive tech) and personal preference. Options included</p> <ul> <li>Reading code as any other default text, meaning word by word.<ul> <li>This was done by participants using their vision and some screen reader users. With this option, it was possible to miss a switch between Markdown and other code cells, though most participants noticed within the first few seconds because their contents are typically distinct. </li> <li>Those using vision mentioned that reading code requires more active reading than recognizing word shapes because code often uses made-up or conjoined words rather than familiar ones; modifying font/font choice becomes critical in allowing participants to manage the strain active reading puts on them.</li> </ul> </li> <li>Reading code character by character.<ul> <li>This was mentioned by participants using screen readers, though it seems probable that participants using their vision would do the same to identify unfamiliar words.</li> <li>Some participants switch to this screen reader setting when they code, others assign this as a preset to a list of applications, and others manually adjust this setting when they feel reading word by word is not serving their needs.</li> </ul> </li> <li>Reading white space in addition to another setting.<ul> <li>This means the white space character\u2019s ( space, tab, etc.) name is read with each repetition.</li> <li>This was only mentioned by screen reader users.</li> <li>Participants that used this method said it can help them identify sections of code outright because the use and frequency of white space is so different than in non-code. This can also help them gain relevant information in code languages where white space is meaningful.</li> </ul> </li> <li>Manually initiating a preset workflow to copy whole blocks of code\u2014like notebook code cells\u2014and paste them into another application that is more accessible to them.<ul> <li>Here they may read, edit, and run code line by line.</li> <li>We did not discuss what they would do if they needed to get code back in the notebook.</li> <li>This was mentioned by the most code-savvy screen reader users, but the workflow is not done on a screen reader itself.</li> </ul> </li> <li>Reading in chunks by a set number of characters, including white space characters.<ul> <li>These chunks do not split at words or other divisions; it is purely by number of characters.</li> <li>This was mentioned by Braille reader users\u2014the number of characters per chunk is determined by the Braille reader\u2019s display.</li> </ul> </li> </ul>"},{"location":"exports/user-tests/2-content/results/cell-contents/#output-errors-and-warnings","title":"Output errors and warnings","text":"<p>Error and warning information appears as additional cell outputs in Jupyter notebooks, they are grouped separately from an intended run cell output. For brevity, we\u2019ll refer to both errors and warnings simply as errors for the rest of this section. </p> <p>Finding and identifying errors was a challenge for most participants and extremely difficult for those not using vision. Error information is almost entirely visual. Other than the error text itself, errors are differianted with padding: a visual break between the section above and below, and low-contrast color coding: a light red or orange background. The color coding was not discernable for those using color inversion or screenreaders. Errors syntax melds directly with the outputs above and below them, so they can be obscured by having a standard text output and error text back to back. Participants using vision, like those with screen magnifiers or high zoom, usually noticed the visual break and thus found errors quickly. Participants with screen readers consistently heard the error message read, but since there was no syntactic break between the error and the other code cells, it was frequently missed. Screen readers read them as one long paragraph rather than as separate text sections. </p> <p>Most participants recognized errors because of their familiarity with the message; they were forced to rely on the content rather than organizational structure, a much slower reading experience. Because words like \u201cerror\u201d or \u201cwarning\u201d often appear in the middle of the message rather than the start, this meant a delay in recognition. This recognition relied on user expertise and prior familiarity with the notebook itself, biasing readability against users newer to the field and those with different cognitive abilities. Some participants with more notebook experience used their knowledge of notebook layout, errors appear after the cell, to search in depth.</p> <p>This comment from a screen reader user summarizes the inaccessible error design's impact: my sighted colleagues can find errors and similar unexpected feedback quickly, but I have to be vigilant and intentionally search to make sure I receive necessary and important information.</p> <p>Participants handled the challenge in a few ways:</p> <ul> <li>Reading all text very carefully and stopping over any area they recognized a keyword (\u201cerror,\u201d \u201cwarning,\u201d or a specific name of an error if they knew the code language).</li> <li>Rereading cells and their outputs when they noticed elsewhere that something was amiss in the notebook.</li> <li>Using the browser find tool to search the notebook\u2019s contents, usually with keywords like \u201cerror\u201d or \u201cwarning.\u201d They navigated using the keyboard.</li> <li>Using a screen reader\u2019s feature to read aloud the background color of a region. This only helps when users first suspect an area is worth investigating. It is not a passive feature and must be intentionally activated by the user.</li> </ul>"},{"location":"exports/user-tests/2-content/results/conclusion/","title":"Issues","text":"<p>Bugs, issues, and other specific feedback or discussions from this round of tests can be found throughout the repository in issues. Listed below, they are</p> <ul> <li>Additional comment on Page title and notebook title do not match</li> <li>Additional comment on Explore landmark options in rendered notebook</li> <li>Syntax highlighting feedback</li> <li>Improve color contrast in rendered notebook</li> <li>Improve errow/warning cell outputs</li> <li>Improve video output experiences</li> <li>Cell navigation feedback</li> </ul>"},{"location":"exports/user-tests/2-content/results/conclusion/#questions-for-future-tests","title":"Questions for future tests","text":"<p>Based on each session and the conversations we had with the development team, we\u2019ve come up with a list of questions we\u2019d like to explore further.</p> <ul> <li>What is the most expected or desirable cell navigation experience? Could there be more than one? What are the different possibilities and how could we best compare them?</li> <li>Should keyboard shortcuts be a part of the rendered notebook experience? Or should we make better use of the browser defaults?</li> <li>How might we increase the discoverability of varied navigation methods in the notebook?</li> <li>What are the limits on cell output types and/or content in notebooks converted to HTML? How does the default accessibility on each type stack up?</li> </ul>"},{"location":"exports/user-tests/2-content/results/conclusion/#what-we-would-do-differently-next-time","title":"What we would do differently next time","text":"<p>Reflecting on these sessions as hosts, for future tests we would like to * Explore a standard notebook with the STScI computer science channels to further discuss ideas for landmarks or other structural options in notebooks. * Experiment with asynchronous test sessions for shorter tasks. This might enable an increased number of and more iterative A/B testing in the long term.</p>"},{"location":"exports/user-tests/2-content/results/navigation/","title":"Navigation: more feedback","text":"<p>Our prior tests were centered around notebook structure and subsequent navigation. Because we addressed some feedback between test sessions and exploring content types does first require navigating to that content, we found further feedback on navigation in this round.</p> <p>Two visibly identical notebooks with different underlying structures were used during these tests. Feedback is sorted by notebook.</p>"},{"location":"exports/user-tests/2-content/results/navigation/#labeled-cells-notebook-structure-1","title":"Labeled cells: notebook structure 1","text":"<p>This notebook structure directly addressed some of the feedback from previous rounds of tests. Headings automatically became links. Execution numbers became their own grouping rather than a portion of the tags encapsulating all content for that cell. <code>div</code> tags were removed wherever possible. Participants reported the following.</p> <ul> <li>This version of the notebook provided some improvement in navigation for most participants. Having headings as links allowed non-screen reader users to take advantage of headings. Having headings as links added headings to the list of interactive elements on the page and made them appear with the tab key. This offered an additional way for screen reader users and keyboard navigators to explore the notebook structure.</li> <li>Requests for a table of contents continued.</li> <li>This version received less feedback overall; it was noted as relatively similar to previous tests.</li> </ul>"},{"location":"exports/user-tests/2-content/results/navigation/#tabable-cells-notebook-structure-2","title":"Tabable cells: notebook structure 2","text":"<p>This notebook structure labeled each cell as an <code>article</code> and leveraged other HTML content categories to provide a more standard HTML structure for assistive tech to hook into. It also made adjustments to low-contrast areas. Participants reported the following.</p> <ul> <li>This version of the notebook received consistently neutral to unfavorable feedback.</li> <li>Non-screen reader users responded neutrally and were unimpacted.<ul> <li>Participants using their vision did give positive feedback on increased color contrast in the execution number and code comments.</li> </ul> </li> <li>Screen reader users were most impacted by these changes and gave negative feedback.<ul> <li>Participants using a screen reader generally needed to be prompted to discover the notebook\u2019s structural changes; the methods it supported them using were not the ways these participants were interested in interacting.</li> <li>Participants successfully used the tab key to navigate through cells (to \u201ctab through cells\u201d). However, populating the tab list with every cell on top of all interactive areas in the notebook created an \u201coverhead of tabs.\u201d One participant described it as \u201cyou don\u2019t know where you are going or what you are looking for. Could be five tabs. Could be fifty\u201d before they can complete the task. This was not considered a positive change.</li> <li>To clarify, the tab key is, by default, a coarse navigation tool that allows users to jump to and from areas where they can perform some kind of interaction. Without this change in notebook structure, which already included all inline links, headings (because they are also links), the video play button, and all browser-level navigation. Making cells tabable adds an additional type of content to filter through in this list, and adds another fifty-plus items to the list in this over-fifty cell notebook.</li> </ul> </li> </ul>"},{"location":"exports/user-tests/2-content/results/navigation/#additional-notes","title":"Additional notes","text":"<p>We found larger UX patterns worth noting. They are listed in no particular order:</p> <ul> <li>There were issues searching and navigating by content type (ie. cell, image, video, so on), but there was a high rate of eventual success. Most tasks were completed by most participants.</li> <li>A common sentiment in tests: \u201cannoying but normal.\u201d Participants expressing this sentiment would first encounter an obstacle they knew how to overcome. They would then report that this obstacle was an everyday occurrence for them across the internet and that the notebook was behaving within the current standard for that user experience. Unfortunately, this was one of the most positive types of feedback we received. It tells us we have a lot of room to grow in making enjoyable and equitable user experiences in both Jupyter and in wider digital spaces.</li> <li>Unlike in the first set of tests for navigation, participants were more likely to miss information or not be able to access it at all. Interestingly, very few participants expressed that they noticed they were missing information; most remained confident they had access to the whole notebook. The few who did observe that they could not access information knew because they found familiar failures\u2014especially images lacking descriptions. </li> <li>Many issues and fixes (requested by participants or found in review) are what might be considered accessibility \u201cbasics.\u201d Alt text/image descriptions, labeling, and contrast issues came up frequently. These are very fixable issues, and they need to be done both in the interface and when authoring individual notebook files.</li> <li>Participants who are more comfortable and/or familiar with Jupyter notebooks expressed more interest in working with the source notebook when encountering obstacles or when trying to find information that wasn\u2019t immediately findable. Filtering through the non-editable version of the notebook was comparatively not worth the effort.</li> <li>Text-based content regularly gave participants fewer issues when compared to non-text content like images or videos. While no content type was without issue, inaccessible images and videos were more likely to block participants completely.</li> <li>Participants using screen magnifiers are especially impacted by the lack of maximum width for notebooks in this form. Because magnifying limits how much information fits on a screen and horizontal scrolling is typically more awkward than vertical scrolling, the full-window line length of notebook content came up as a serious pain point and contributor to fatigue. It also increased the risk of screen magnifier users missing information, especially on the right-hand side (for a notebook in English, a left-to-right language).</li> <li>Some participants would complete or describe completing tasks using an ability that fatigued or even hurt them. For example, participants with low vision strained to use their vision to complete a task that their assistive tech was unable to work with (due to poor infrastructure or tagging on Jupyter\u2019s part). Yet another way that inaccessibility harms people who are determined to work in fields that rely heavily on notebooks.</li> <li>Jupyter notebooks often bring together many types of content, and this content can bring its own accessibility issues with it. Notebooks have the capacity to inherit accessibility problems from everything that makes them up\u2014from Jupyter-maintained tools to any other package. For these tests, we ran into issues like lack of image description support for plotting packages, lack of labeling in the embedded video player and its buttons, and low contrast syntax highlighting themes. On the Jupyter side, we can also make choices about what packages to support or how we handle these inaccessible defaults. Notebooks can surface inaccessibility from anywhere.</li> <li>Authors will continue to have a large amount of power to determine the accessibility of an individual document. This is part of why we are drafting authoring recommendations.</li> <li>Participants search for familiarity to anchor their experience. What was familiar to each participant varied depending on their field of expertise, accessibility accommodations used, what other software they were familiar with, and Jupyter notebook experience specifically. Examples include:<ul> <li>Participants who are familiar with Jupyter notebooks would more often talk about cells and try and find ways to distinguish between them. They also were the only participants who called out insufficient divisions and information to find cells.</li> <li>Participants using screen readers were more likely to expect content headings to be more robust. These participants were also more likely to explain their mental model of cells (or other divides) in the notebook by the idea of headings. </li> <li>Participants used to working with editable versions of notebooks or other source code forms were more likely to compare behaviors to an editable document and asked to have those experiences carry over. For example, some participants wanted to be able to navigate content by editable versus non-editable areas to tell the difference between cell inputs and outputs.</li> <li>Error and warning outputs\u2014an (often unexpected) cell output that reports to users when something in the code run is not functioning as expected\u2014were only findable because some participants knew to expect one. Many participants missed the text-only transition to an error message in the test notebook because it had no other indicators. As it was a common error, some participants clocked into it immediately and without host support, but they reported it was only because they had heard that exact sentence many times before.</li> </ul> </li> </ul>"},{"location":"exports/user-tests/2-content/results/rich-outputs/","title":"Images","text":""},{"location":"exports/user-tests/2-content/results/rich-outputs/#png-images","title":"PNG images","text":"<p>PNG images proved an obstacle for most participants. Different notebook authoring choices can be employed to fix many of these issues.</p> <p>The primary issue with images is that they provide information in an inflexible way. Participants using vision can magnify or zoom into an image (though browser zoom does not apply), but that is the only control participants were able to exercise over images. If these same participants cannot get information because the image has areas that are too low contrast, there is nothing they can do; any theme changes or color inversions they may apply elsewhere will not apply to an image. Because of this, low contrast is an extreme blocker for participants to complete tasks related to images.</p> <p>This notebook provides absolutely no support for participants not using vision. Because the images in this notebook had no alt text or image descriptions, were not described in surrounding context, and provided no ways to access the source information, screen reader and Braille reader users could not complete any image-related tasks in these tests. At most, participants would take a guess based on the image file\u2019s name\u2014the only information their assistive tech could access. </p> <p>To manage this poor experience, all participants:</p> <ul> <li>Would read before and after the image to try and glean the image\u2019s surrounding context.</li> <li>Would search for any links that might send them to an image source, related data, or provide other context.</li> </ul> <p>Participants using their vision also:</p> <ul> <li>Tried to magnify or zoom in on the image. Which methods of magnification or zoom they tired first depended on their personal preference and amount of zoom needed. Remember that browser zoom does not zoom the image in a rendered notebook.</li> <li>Would adjust their display settings, like using a high contrast mode or inverting colors. These did not successfully impact images.</li> </ul> <p>Participants using screen readers or Braille readers also:</p> <ul> <li>Would explore more aggressively when searching for image information and often noted there was no other recourse. They were locked out from the information in an undescribed image.</li> </ul>"},{"location":"exports/user-tests/2-content/results/rich-outputs/#svg-images","title":"SVG images","text":"<p>With a default\u2014meaning not manually tagged or otherwise modified\u2014SVG image, participants noted no differences between the experience of the PNG plots versus the SVG plots in this notebook.</p>"},{"location":"exports/user-tests/2-content/results/rich-outputs/#chartvisualization-feedback","title":"Chart/visualization feedback","text":"<p>In the notebook used for these tests, a majority of the images were charts or other kinds of visualizations. We received the following feedback on charts used during the test:</p> <ul> <li>Missing chart information is confusing at best and misleading at worst. Participants often struggled to make sense of what the charts were trying to explain to them because several were missing titles or axis labels. Not including these fundamental aspects has direct negative impact on readability.</li> <li>In most cases, summarizing or including a description of the information a chart is meant to convey can help participants dive into the information faster and more deeply. If done as a text description, this may also serve as support for screen reader and Braille users.</li> <li>Default styling of plots were some of the biggest contrast obstacles for participants engaging with the notebook visually. The default styling often had low contrast color choices representing the data, thin and low opacity lines for trend-lines and axes, and small and thin text. Because they are images, colors and lines and text are not customizable or restylable even for participants who wanted to try editing them using developer tools. </li> <li>Use grid lines in charts. When using high magnification, zoom, or otherwise handling a limited field of vision, participants using their vision often could not see both the axes and the data at the same time. Without gridlines, they had nothing to follow to orient that data point in relation to the axes.</li> <li>Including the tables of data used to create complex charts was an unpleasant but reliable way of accessing the information in the plot. This was the same answer whether tables were included in the notebook already or if users would have to generate one themselves. For participants more experienced with data analysis, having access to the source notebook and data was consistently preferable.</li> </ul>"},{"location":"exports/user-tests/2-content/results/rich-outputs/#videos","title":"Videos","text":"<p>The <code>iframe</code> linked video in our test notebook was mostly usable by participants who completed tasks relating to the video more often than not. When asked to reflect on the experience participants credited the fact that the video and its interface were easier to use because it followed video patterns they expected and experienced elsewhere on the internet. </p> <p>Video feedback included:</p> <ul> <li>The video did not immediately appear to be a video.<ul> <li>Participants using vision mistook it for an image because the video player around it does not appear until a user interacts with it.</li> <li>Participants using screen readers commented that the video did not have any kind of labeling that told them it was a video; they figured it out when they found the familiar play button (though it did not tell them whether that would be audio or video). This is likely a result of the <code>iframe</code> and no additional labeling.</li> </ul> </li> <li>Participants would like closed caption and/or transcription options.</li> <li>The area the video took up was unclear.<ul> <li>This became an issue for participants using their vision when they were trying to figure out where they could click to pause and play the video.</li> <li>When magnified or zoomed in, it could also become difficult to tell which parts were video and which parts were notebook background. </li> </ul> </li> <li>Because the video player does not appear around the video until the video is played, its controls are unclear. The initial play button that appears as an overlay of the video thumbnail does not seem to be labeled as a play button; screen reader users were guessing when they activated the button.</li> </ul>"},{"location":"exports/user-tests/2-content/results/rich-outputs/#content-types-not-covered","title":"Content types not covered","text":"<p>While we aimed to cover a breadth of commonly used content types in these tests, we could not cover everything that could possibly be put in a notebook\u2019s cells. For this round of tests, we intentionally did not focus on</p> <ul> <li>External links</li> <li>Tables</li> <li>Iframes</li> <li>Interactive Widgets</li> </ul> <p>The decision to not focus these was motivated by constraints like the length of a testing session (one hour per participant), the STScI notebooks available to us, the content types most commonly found in public-facing STScI notebooks, prioritizing content types that had received no feedback in prior sessions, and interactivity limits in a rendered notebook.</p>"},{"location":"exports/user-tests/3-cell_structure/planning/","title":"Cell Structure Exploration in Rendered Notebooks","text":"<p>This set of tests has not been run. Because the team spent time discussing the logistics of this potential test, we are collecting the notes here so the work remains public.</p>"},{"location":"exports/user-tests/3-cell_structure/planning/#things-needed","title":"Things needed","text":"<ul> <li> STScI Notebook(s) (at least one, maybe three)</li> <li> Hosted HTML rendered notebooks with the different cell structures mentioned in the April meeting notes (list, feed, table, treegrid, grid)</li> <li> List of tasks to complete per notebook</li> <li> Script and feedback format</li> <li> Logistic and payment paperwork</li> </ul>"},{"location":"exports/user-tests/3-cell_structure/planning/#example-of-script-structure","title":"Example of script structure","text":"<ul> <li> <p> Task 1: Open notebook</p> <ul> <li> I could open the notebook</li> <li> I could not open the notebook</li> <li> Something else (please describe)</li> </ul> </li> <li> <p>How did you know when you made it to the notebook? </p> </li> <li> <p>What controls did you use? (ie. mouse, keyboard shortcut, dictation)</p> </li> <li> <p> Task 2: Find and read the Introduction. Who is the author of this notebook?</p> <ul> <li> I could find the Introduction. The author is:</li> <li> I could not find the Introdcution</li> <li> Something else (please describe)</li> </ul> </li> <li> <p>What controls did you use? (ie. mouse, keyboard shortcut, dictation)</p> </li> <li> <p> Task 3: Reflect on how the notebooks worked for you</p> </li> <li> <p>Did you prefer notebook 1 or 2? Why?</p> </li> <li>How do wish cell navigation worked in these notebooks? What would be the ideal steps for you?</li> <li>What was your favorite thing that happened in this session?</li> <li>What was your least favorite thing that happened in this session? </li> </ul>"},{"location":"exports/user-tests/3-cell_structure/planning/#other-test-logistics","title":"Other test logistics","text":"<ul> <li>This is meant to be run as a asynchronous test where we send a list of tasks for participants to complete in an hour on their own.</li> <li>How do we want to compare cell structures? <ul> <li>Because we have a small sample with very different means of traversing notebooks, a traditional A/B test where only you split the sample between two different experiences seems unfeasible.</li> <li>Could each participant complete the same tasks in two versions of the same notebook then compare?</li> <li>Could each participant complete the same tasks in two notebooks with two different cell structures?</li> <li>Could each particpant complete the same tasks in the two notebooks made my reshuffling the cells from the same notebook with different cell structures? This could help remove familiarity of completing the tasks in the same notebook a second time.</li> </ul> </li> <li>We can account for the factors of self-reported feedback by providing options for participants to choose from then add to rather than asking all open questions like we do synchronously. </li> </ul>"}]}